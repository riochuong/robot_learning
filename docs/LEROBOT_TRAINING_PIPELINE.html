<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LeRobot Training Pipeline Deep Dive</title>
    <style>
        :root {
            --bg-primary: #0d1117;
            --bg-secondary: #161b22;
            --bg-tertiary: #21262d;
            --text-primary: #e6edf3;
            --text-secondary: #8b949e;
            --accent-blue: #58a6ff;
            --accent-green: #3fb950;
            --accent-purple: #a371f7;
            --accent-orange: #f0883e;
            --accent-red: #f85149;
            --accent-yellow: #d29922;
            --accent-pink: #db61a2;
            --border-color: #30363d;
            --code-bg: #1c2128;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', -apple-system, BlinkMacSystemFont, sans-serif;
            background: var(--bg-primary);
            color: var(--text-primary);
            line-height: 1.7;
            padding: 40px;
            max-width: 1400px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2.5rem;
            margin-bottom: 10px;
            background: linear-gradient(135deg, var(--accent-blue), var(--accent-purple));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        h2 {
            font-size: 1.8rem;
            color: var(--accent-blue);
            margin: 40px 0 20px 0;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--border-color);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--accent-green);
            margin: 30px 0 15px 0;
        }

        h4 {
            font-size: 1.2rem;
            color: var(--accent-purple);
            margin: 20px 0 10px 0;
        }

        h5 {
            font-size: 1rem;
            color: var(--accent-orange);
            margin: 15px 0 8px 0;
        }

        p {
            margin: 15px 0;
            color: var(--text-primary);
        }

        .subtitle {
            font-size: 1.2rem;
            color: var(--text-secondary);
            margin-bottom: 30px;
        }

        .toc {
            background: var(--bg-secondary);
            border: 1px solid var(--border-color);
            border-radius: 12px;
            padding: 25px;
            margin: 30px 0;
        }

        .toc h3 {
            color: var(--accent-blue);
            margin: 0 0 15px 0;
        }

        .toc ul {
            list-style: none;
        }

        .toc li {
            margin: 8px 0;
        }

        .toc a {
            color: var(--text-secondary);
            text-decoration: none;
            transition: color 0.2s;
        }

        .toc a:hover {
            color: var(--accent-blue);
        }

        pre {
            background: var(--code-bg);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            overflow-x: auto;
            margin: 15px 0;
            font-family: 'JetBrains Mono', 'Fira Code', 'Consolas', monospace;
            font-size: 0.85rem;
            line-height: 1.6;
        }

        code {
            font-family: 'JetBrains Mono', 'Fira Code', 'Consolas', monospace;
            font-size: 0.9em;
        }

        :not(pre) > code {
            background: var(--bg-tertiary);
            padding: 2px 6px;
            border-radius: 4px;
            color: var(--accent-orange);
        }

        .model-section {
            background: var(--bg-secondary);
            border: 1px solid var(--border-color);
            border-radius: 12px;
            padding: 25px;
            margin: 25px 0;
        }

        .model-header {
            display: flex;
            align-items: center;
            gap: 15px;
            margin-bottom: 20px;
        }

        .model-icon {
            font-size: 2rem;
        }

        .pipeline-diagram {
            background: var(--bg-tertiary);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin: 20px 0;
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.85rem;
            overflow-x: auto;
            white-space: pre;
        }

        .flow-step {
            display: inline-block;
            background: var(--bg-tertiary);
            border: 1px solid var(--accent-blue);
            border-radius: 6px;
            padding: 8px 15px;
            margin: 5px;
            font-size: 0.9rem;
        }

        .flow-arrow {
            color: var(--accent-green);
            font-weight: bold;
            margin: 0 8px;
        }

        .info-box {
            background: var(--bg-tertiary);
            border-left: 4px solid var(--accent-blue);
            padding: 15px 20px;
            margin: 20px 0;
            border-radius: 0 8px 8px 0;
        }

        .warning-box {
            background: var(--bg-tertiary);
            border-left: 4px solid var(--accent-orange);
            padding: 15px 20px;
            margin: 20px 0;
            border-radius: 0 8px 8px 0;
        }

        .key-concept {
            background: linear-gradient(135deg, rgba(88, 166, 255, 0.1), rgba(163, 113, 247, 0.1));
            border: 1px solid var(--accent-purple);
            border-radius: 8px;
            padding: 20px;
            margin: 20px 0;
        }

        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            font-size: 0.9rem;
        }

        .comparison-table th,
        .comparison-table td {
            border: 1px solid var(--border-color);
            padding: 12px 15px;
            text-align: left;
        }

        .comparison-table th {
            background: var(--bg-tertiary);
            color: var(--accent-blue);
            font-weight: 600;
            position: sticky;
            top: 0;
        }

        .comparison-table tr:hover {
            background: var(--bg-tertiary);
        }

        .tensor-shape {
            color: var(--accent-green);
            font-family: monospace;
        }

        .keyword { color: var(--accent-purple); }
        .string { color: var(--accent-green); }
        .number { color: var(--accent-orange); }
        .comment { color: var(--text-secondary); font-style: italic; }
        .function { color: var(--accent-blue); }
        .class { color: var(--accent-yellow); }

        ul, ol {
            margin: 15px 0 15px 25px;
        }

        li {
            margin: 8px 0;
        }

        .architecture-box {
            background: var(--bg-secondary);
            border: 1px solid var(--border-color);
            border-radius: 12px;
            padding: 30px;
            margin: 25px 0;
            overflow-x: auto;
        }

        .arch-diagram {
            display: flex;
            flex-direction: column;
            gap: 20px;
            font-family: 'Segoe UI', sans-serif;
        }

        .arch-row {
            display: flex;
            align-items: center;
            gap: 15px;
            flex-wrap: wrap;
        }

        .arch-box {
            background: var(--bg-tertiary);
            border: 2px solid var(--border-color);
            border-radius: 8px;
            padding: 15px 20px;
            min-width: 180px;
            text-align: center;
            position: relative;
            transition: all 0.2s;
        }

        .arch-box:hover {
            border-color: var(--accent-blue);
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(88, 166, 255, 0.2);
        }

        .arch-box-title {
            font-weight: 600;
            font-size: 0.95rem;
            margin-bottom: 8px;
            color: var(--text-primary);
        }

        .arch-box-desc {
            font-size: 0.8rem;
            color: var(--text-secondary);
            line-height: 1.4;
        }

        .arch-box-shape {
            font-size: 0.75rem;
            color: var(--accent-green);
            font-family: 'JetBrains Mono', monospace;
            margin-top: 5px;
        }

        .arch-arrow {
            color: var(--accent-green);
            font-size: 1.5rem;
            font-weight: bold;
            flex-shrink: 0;
        }

        .arch-arrow-horizontal {
            transform: rotate(90deg);
        }

        .arch-merge {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 10px;
        }

        .arch-merge-line {
            width: 2px;
            height: 30px;
            background: var(--accent-blue);
        }

        .arch-section {
            background: linear-gradient(135deg, rgba(88, 166, 255, 0.05), rgba(163, 113, 247, 0.05));
            border: 2px dashed var(--accent-purple);
            border-radius: 8px;
            padding: 20px;
            margin: 15px 0;
        }

        .arch-section-title {
            font-weight: 600;
            color: var(--accent-purple);
            margin-bottom: 15px;
            text-align: center;
            font-size: 1rem;
        }

        .arch-box.blue { border-color: var(--accent-blue); }
        .arch-box.blue .arch-box-title { color: var(--accent-blue); }
        .arch-box.green { border-color: var(--accent-green); }
        .arch-box.green .arch-box-title { color: var(--accent-green); }
        .arch-box.purple { border-color: var(--accent-purple); }
        .arch-box.purple .arch-box-title { color: var(--accent-purple); }
        .arch-box.orange { border-color: var(--accent-orange); }
        .arch-box.orange .arch-box-title { color: var(--accent-orange); }
        .arch-box.pink { border-color: var(--accent-pink); }
        .arch-box.pink .arch-box-title { color: var(--accent-pink); }
        .arch-box.yellow { border-color: var(--accent-yellow); }
        .arch-box.yellow .arch-box-title { color: var(--accent-yellow); }

        .highlight-blue { color: var(--accent-blue); }
        .highlight-green { color: var(--accent-green); }
        .highlight-purple { color: var(--accent-purple); }
        .highlight-orange { color: var(--accent-orange); }
        .highlight-pink { color: var(--accent-pink); }
    </style>
</head>
<body>
    <h1>ğŸ¤– LeRobot Training Pipeline Deep Dive</h1>
    <p class="subtitle">Understanding how data flows from dataset to model for SmolVLA, Pi0, ACT, Groot, X-VLA, and Diffusion policies</p>

    <div class="toc">
        <h3>ğŸ“‘ Table of Contents</h3>
        <ul>
            <li><a href="#overview">1. Overview: The Training Loop</a></li>
            <li><a href="#dataloader">2. DataLoader & Dataset Loading</a></li>
            <li><a href="#preprocessing">3. Preprocessing Pipeline</a></li>
            <li><a href="#smolvla">4. SmolVLA: Vision-Language-Action Model</a></li>
            <li><a href="#pi0">5. Pi0: PaliGemma + Expert Model</a></li>
            <li><a href="#pi05">6. Pi0.5: No State, AdaRMS Conditioning</a></li>
            <li><a href="#pi05-fast">7. Pi0.5-Fast: Optimized for Speed</a></li>
            <li><a href="#act">8. ACT: Action Chunking Transformer</a></li>
            <li><a href="#groot">9. Groot: NVIDIA GR00T Integration</a></li>
            <li><a href="#diffusion">10. Diffusion Policy: Denoising Actions</a></li>
            <li><a href="#xvla">11. X-VLA: Cross-Embodiment VLA</a></li>
            <li><a href="#comparison">12. Model Comparison Summary</a></li>
        </ul>
    </div>

    <!-- ============================================ -->
    <!-- SECTION 1: Overview -->
    <!-- ============================================ -->
    <h2 id="overview">ğŸ”„ 1. Overview: The Training Loop</h2>

    <p>The LeRobot training pipeline follows a consistent pattern across all policies. Here's the high-level flow:</p>

    <div class="pipeline-diagram">
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           LEROBOT TRAINING PIPELINE                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Dataset    â”‚â”€â”€â”€â”€â–ºâ”‚   DataLoader    â”‚â”€â”€â”€â”€â–ºâ”‚  Preprocessor  â”‚â”€â”€â”€â”€â–ºâ”‚   Policy   â”‚ â”‚
â”‚  â”‚ (Parquet +   â”‚     â”‚ (Batching +     â”‚     â”‚ (Normalize +   â”‚     â”‚  .forward()â”‚ â”‚
â”‚  â”‚  Videos)     â”‚     â”‚  Shuffling)     â”‚     â”‚  Tokenize)     â”‚     â”‚            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                            â”‚        â”‚
â”‚                                                                            â–¼        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Metrics    â”‚â—„â”€â”€â”€â”€â”‚    Optimizer    â”‚â—„â”€â”€â”€â”€â”‚   Loss Func    â”‚â—„â”€â”€â”€â”€â”‚  Predicted â”‚ â”‚
â”‚  â”‚   & Logs     â”‚     â”‚  (AdamW + LR    â”‚     â”‚   (L1/MSE/     â”‚     â”‚   Actions  â”‚ â”‚
â”‚  â”‚              â”‚     â”‚   Scheduler)    â”‚     â”‚   Flow Match)  â”‚     â”‚            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    </div>

    <h3>Training Script Entry Point</h3>
    <p>The main training loop is in <code>src/lerobot/scripts/lerobot_train.py</code>:</p>

    <pre><code><span class="comment"># Simplified training loop from lerobot_train.py</span>
<span class="keyword">for</span> _ <span class="keyword">in</span> range(step, cfg.steps):
    <span class="comment"># 1. Load batch from DataLoader</span>
    batch = next(dl_iter)
    
    <span class="comment"># 2. Apply preprocessing (normalize, tokenize, move to device)</span>
    batch = preprocessor(batch)
    
    <span class="comment"># 3. Forward pass through policy</span>
    loss, output_dict = policy.forward(batch)
    
    <span class="comment"># 4. Backward pass + optimizer step</span>
    accelerator.backward(loss)
    optimizer.step()
    lr_scheduler.step()
    optimizer.zero_grad()
    
    <span class="comment"># 5. Logging and checkpointing</span>
    <span class="keyword">if</span> step % cfg.save_freq == <span class="number">0</span>:
        save_checkpoint(...)</code></pre>

    <!-- ============================================ -->
    <!-- SECTION 2: DataLoader -->
    <!-- ============================================ -->
    <h2 id="dataloader">ğŸ“¦ 2. DataLoader & Dataset Loading</h2>

    <h3>LeRobotDataset.__getitem__</h3>
    <p>When the DataLoader requests a sample, <code>LeRobotDataset.__getitem__</code> is called:</p>

    <pre><code><span class="keyword">def</span> <span class="function">__getitem__</span>(<span class="keyword">self</span>, idx) -> dict:
    <span class="comment"># 1. Load tabular data from HuggingFace dataset (Parquet)</span>
    item = <span class="keyword">self</span>.hf_dataset[idx]
    ep_idx = item[<span class="string">"episode_index"</span>].item()

    <span class="comment"># 2. Handle temporal sequences (delta_indices for past/future frames)</span>
    <span class="keyword">if</span> <span class="keyword">self</span>.delta_indices <span class="keyword">is not None</span>:
        query_indices, padding = <span class="keyword">self</span>._get_query_indices(idx, ep_idx)
        query_result = <span class="keyword">self</span>._query_hf_dataset(query_indices)
        item = {**item, **padding}  <span class="comment"># Add padding masks</span>
        <span class="keyword">for</span> key, val <span class="keyword">in</span> query_result.items():
            item[key] = val

    <span class="comment"># 3. Decode video frames for visual observations</span>
    <span class="keyword">if</span> len(<span class="keyword">self</span>.meta.video_keys) > <span class="number">0</span>:
        current_ts = item[<span class="string">"timestamp"</span>].item()
        query_timestamps = <span class="keyword">self</span>._get_query_timestamps(current_ts, query_indices)
        video_frames = <span class="keyword">self</span>._query_videos(query_timestamps, ep_idx)
        item = {**video_frames, **item}

    <span class="comment"># 4. Apply image transforms (augmentation)</span>
    <span class="keyword">if</span> <span class="keyword">self</span>.image_transforms <span class="keyword">is not None</span>:
        <span class="keyword">for</span> cam <span class="keyword">in</span> <span class="keyword">self</span>.meta.camera_keys:
            item[cam] = <span class="keyword">self</span>.image_transforms(item[cam])

    <span class="keyword">return</span> item</code></pre>

    <h3>What the DataLoader Returns</h3>
    <p>A typical batch from the DataLoader contains:</p>

    <table class="comparison-table">
        <thead>
            <tr>
                <th>Key</th>
                <th>Shape</th>
                <th>Description</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><code>observation.state</code></td>
                <td class="tensor-shape">(B, n_obs_steps, state_dim)</td>
                <td>Robot joint positions, velocities, etc.</td>
            </tr>
            <tr>
                <td><code>observation.images.cam1</code></td>
                <td class="tensor-shape">(B, n_obs_steps, C, H, W)</td>
                <td>Camera images (normalized to [0, 1])</td>
            </tr>
            <tr>
                <td><code>action</code></td>
                <td class="tensor-shape">(B, chunk_size, action_dim)</td>
                <td>Target action sequence (ground truth)</td>
            </tr>
            <tr>
                <td><code>action_is_pad</code></td>
                <td class="tensor-shape">(B, chunk_size)</td>
                <td>Boolean mask for padded actions</td>
            </tr>
            <tr>
                <td><code>task</code></td>
                <td>List[str] of length B</td>
                <td>Language task description</td>
            </tr>
            <tr>
                <td><code>timestamp</code></td>
                <td class="tensor-shape">(B,)</td>
                <td>Frame timestamp</td>
            </tr>
            <tr>
                <td><code>episode_index</code></td>
                <td class="tensor-shape">(B,)</td>
                <td>Episode identifier</td>
            </tr>
        </tbody>
    </table>

    <!-- ============================================ -->
    <!-- SECTION 3: Preprocessing -->
    <!-- ============================================ -->
    <h2 id="preprocessing">âš™ï¸ 3. Preprocessing Pipeline</h2>

    <p>Each policy has a <code>make_*_pre_post_processors()</code> function that creates a preprocessing pipeline. Common steps include:</p>

    <div class="pipeline-diagram">
Raw Batch from DataLoader
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. RenameObservationsProcessorStep â”‚  Rename keys to match pretrained config
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. AddBatchDimensionProcessorStep  â”‚  Ensure batch dim exists (for inference)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. TokenizerProcessorStep          â”‚  Convert task string â†’ token IDs (VLA models)
â”‚    (SmolVLA, Pi0 only)             â”‚  Output: observation.language_tokens
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. DeviceProcessorStep             â”‚  Move tensors to GPU (cuda/mps)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. NormalizerProcessorStep         â”‚  Normalize features based on dataset stats
â”‚    - MEAN_STD: (x - mean) / std    â”‚
â”‚    - MIN_MAX: (x - min) / (max-min)â”‚
â”‚    - IDENTITY: no normalization    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    Preprocessed Batch
    </div>

    <h3>Normalization Modes</h3>
    <pre><code><span class="comment"># Example normalization_mapping from SmolVLAConfig</span>
normalization_mapping = {
    <span class="string">"VISUAL"</span>: NormalizationMode.IDENTITY,     <span class="comment"># Images: no normalization (already [0,1])</span>
    <span class="string">"STATE"</span>: NormalizationMode.MEAN_STD,       <span class="comment"># State: (x - Î¼) / Ïƒ</span>
    <span class="string">"ACTION"</span>: NormalizationMode.MEAN_STD,      <span class="comment"># Actions: (x - Î¼) / Ïƒ</span>
}</code></pre>

    <!-- ============================================ -->
    <!-- SECTION 4: SmolVLA -->
    <!-- ============================================ -->
    <h2 id="smolvla">ğŸ¦™ 4. SmolVLA: Vision-Language-Action Model</h2>

    <div class="model-section">
        <div class="model-header">
            <span class="model-icon">ğŸ”®</span>
            <div>
                <h3 style="margin: 0; color: var(--accent-pink);">SmolVLA Architecture</h3>
                <p style="margin: 5px 0 0 0; color: var(--text-secondary);">Lightweight VLM backbone + Action Expert with Flow Matching</p>
            </div>
        </div>

        <div class="architecture-box">
            <div class="arch-diagram">
                <!-- Input Layer -->
                <div class="arch-row" style="justify-content: space-around;">
                    <div class="arch-box blue">
                        <div class="arch-box-title">Images</div>
                        <div class="arch-box-shape">(B, num_cams, C, H, W)</div>
                    </div>
                    <div class="arch-box green">
                        <div class="arch-box-title">Language</div>
                        <div class="arch-box-shape">(B, seq_len)</div>
                    </div>
                    <div class="arch-box orange">
                        <div class="arch-box-title">State</div>
                        <div class="arch-box-shape">(B, state_dim)</div>
                    </div>
                </div>

                <!-- Arrow Down -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-arrow">â†“</div>
                </div>

                <!-- Encoding Layer -->
                <div class="arch-row" style="justify-content: space-around;">
                    <div class="arch-box blue">
                        <div class="arch-box-title">SigLIP Vision Encoder</div>
                        <div class="arch-box-desc">Extract visual features</div>
                    </div>
                    <div class="arch-box green">
                        <div class="arch-box-title">Tokenizer (SmolVLM)</div>
                        <div class="arch-box-desc">Tokenize language</div>
                    </div>
                    <div class="arch-box orange">
                        <div class="arch-box-title">State Projection</div>
                        <div class="arch-box-desc">Linear projection</div>
                    </div>
                </div>

                <!-- Arrow Down -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-arrow">â†“</div>
                </div>

                <!-- Projection Layer -->
                <div class="arch-row" style="justify-content: space-around;">
                    <div class="arch-box blue">
                        <div class="arch-box-title">Vision Projection</div>
                        <div class="arch-box-desc">(Connector)</div>
                    </div>
                    <div class="arch-box green">
                        <div class="arch-box-title">Embed Tokens</div>
                        <div class="arch-box-desc">LLM embedding layer</div>
                    </div>
                    <div class="arch-box orange">
                        <div class="arch-box-title">State Embedding</div>
                        <div class="arch-box-desc">Projected state</div>
                    </div>
                </div>

                <!-- Merge Arrow -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-merge">
                        <div class="arch-merge-line"></div>
                        <div class="arch-arrow">â†“</div>
                    </div>
                </div>

                <!-- Prefix Embeddings Section -->
                <div class="arch-section">
                    <div class="arch-section-title">PREFIX EMBEDDINGS</div>
                    <div class="arch-box purple" style="max-width: 100%;">
                        <div class="arch-box-title">[img_embâ‚, img_embâ‚‚, ..., lang_emb, state_emb]</div>
                        <div class="arch-box-shape">Shape: (B, prefix_len, hidden_dim)</div>
                    </div>
                </div>

                <!-- Arrow Down -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-arrow">â†“</div>
                </div>

                <!-- SmolVLM Transformer -->
                <div class="arch-box blue" style="max-width: 100%;">
                    <div class="arch-box-title">SmolVLM Transformer</div>
                    <div class="arch-box-desc">â€¢ Processes prefix (images + language + state)</div>
                    <div class="arch-box-desc">â€¢ Generates KV cache for action expert</div>
                    <div class="arch-box-desc">â€¢ Frozen during finetuning (train_expert_only=True)</div>
                </div>

                <!-- Arrow Down -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-arrow">â†“</div>
                    <div style="margin-left: 10px; color: var(--text-secondary); font-size: 0.85rem;">KV Cache</div>
                </div>

                <!-- Action Expert -->
                <div class="arch-box orange" style="max-width: 100%;">
                    <div class="arch-box-title">ACTION EXPERT</div>
                    <div class="arch-box-desc">â€¢ Input: noisy_actions + timestep embedding</div>
                    <div class="arch-box-desc">â€¢ Cross-attention to VLM's KV cache</div>
                    <div class="arch-box-desc">â€¢ Predicts velocity field (for flow matching)</div>
                </div>

                <!-- Arrow Down -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-arrow">â†“</div>
                </div>

                <!-- Output Projection -->
                <div class="arch-box green" style="max-width: 100%;">
                    <div class="arch-box-title">OUTPUT PROJECTION</div>
                    <div class="arch-box-desc">action_out_proj: Linear(expert_hidden, max_action_dim)</div>
                    <div class="arch-box-shape">Output: (B, chunk_size, max_action_dim)</div>
                </div>
            </div>
        </div>

        <h4>Step-by-Step Forward Pass (Training)</h4>

        <h5>Step 1: Prepare Images</h5>
        <pre><code><span class="keyword">def</span> <span class="function">prepare_images</span>(<span class="keyword">self</span>, batch):
    <span class="string">"""Resize to 512x512 with padding, normalize to [-1, 1] for SigLIP."""</span>
    images = []
    <span class="keyword">for</span> key <span class="keyword">in</span> <span class="keyword">self</span>.config.image_features:
        img = batch[key][:, -<span class="number">1</span>, :, :, :]  <span class="comment"># Take last observation step: (B, C, H, W)</span>
        
        <span class="comment"># Resize with padding to maintain aspect ratio</span>
        img = resize_with_pad(img, <span class="number">512</span>, <span class="number">512</span>, pad_value=<span class="number">0</span>)
        
        <span class="comment"># Normalize from [0,1] to [-1,1] as expected by SigLIP</span>
        img = img * <span class="number">2.0</span> - <span class="number">1.0</span>
        
        images.append(img)
    <span class="keyword">return</span> images, img_masks</code></pre>

        <h5>Step 2: Prepare State (Pad to max_state_dim)</h5>
        <pre><code><span class="keyword">def</span> <span class="function">prepare_state</span>(<span class="keyword">self</span>, batch):
    <span class="string">"""Pad state vector to max_state_dim (default: 32)."""</span>
    state = batch[<span class="string">"observation.state"</span>][:, -<span class="number">1</span>, :]  <span class="comment"># (B, state_dim)</span>
    state = pad_vector(state, <span class="keyword">self</span>.config.max_state_dim)  <span class="comment"># (B, 32)</span>
    <span class="keyword">return</span> state</code></pre>

        <h5>Step 3: Embed Prefix (Images + Language + State)</h5>
        <div class="info-box">
            <strong>ğŸ“ SmolVLA State Position:</strong> State is embedded in the <strong>PREFIX</strong> (with images and language). This is different from Pi0 where state goes in the SUFFIX.
        </div>
        <pre><code><span class="keyword">def</span> <span class="function">embed_prefix</span>(<span class="keyword">self</span>, images, img_masks, lang_tokens, lang_masks, state):
    embs = []
    att_masks = []
    
    <span class="comment"># 3a. Embed images with SigLIP vision encoder</span>
    <span class="keyword">for</span> img <span class="keyword">in</span> images:
        img_emb = <span class="keyword">self</span>.vlm_with_expert.embed_image(img)  <span class="comment"># (B, num_patches, hidden_dim)</span>
        img_emb = img_emb * sqrt(hidden_dim)  <span class="comment"># Scale normalization</span>
        embs.append(img_emb)
        att_masks += [<span class="number">0</span>] * num_patches  <span class="comment"># Can attend to images</span>
    
    <span class="comment"># 3b. Embed language tokens</span>
    lang_emb = <span class="keyword">self</span>.vlm_with_expert.embed_language_tokens(lang_tokens)  <span class="comment"># (B, seq_len, hidden_dim)</span>
    lang_emb = lang_emb * sqrt(hidden_dim)
    embs.append(lang_emb)
    att_masks += [<span class="number">0</span>] * num_lang_tokens
    
    <span class="comment"># 3c. Embed state with linear projection (STATE IS IN PREFIX!)</span>
    state_emb = <span class="keyword">self</span>.state_proj(state)  <span class="comment"># Linear(32, hidden_dim) â†’ (B, 1, hidden_dim)</span>
    embs.append(state_emb)
    att_masks += [<span class="number">1</span>]  <span class="comment"># State/actions don't attend to img/lang (causal masking)</span>
    
    <span class="comment"># 3d. Concatenate all prefix embeddings</span>
    prefix_embs = torch.cat(embs, dim=<span class="number">1</span>)  <span class="comment"># (B, prefix_len, hidden_dim)</span>
    <span class="comment"># PREFIX = [img_embs, lang_embs, state_emb]</span>
    
    <span class="keyword">return</span> prefix_embs, pad_masks, att_masks</code></pre>

        <h5>Step 3b: Embed Suffix (Actions + Timestep - NO State)</h5>
        <div class="info-box">
            <strong>ğŸ“ SmolVLA Suffix:</strong> The suffix contains only <code>noisy_actions + timestep</code>. State is NOT included in the suffix (unlike Pi0).
        </div>
        <pre><code><span class="keyword">def</span> <span class="function">embed_suffix</span>(<span class="keyword">self</span>, noisy_actions, timestep):
    <span class="comment"># Note: NO state parameter in embed_suffix for SmolVLA</span>
    <span class="comment"># Fuse timestep + action information using an MLP</span>
    action_emb = <span class="keyword">self</span>.action_in_proj(noisy_actions)
    time_emb = create_sinusoidal_pos_embedding(timestep, hidden_dim, ...)
    action_time_emb = MLP([action_emb, time_emb])
    <span class="comment"># SUFFIX = [action_time_emb] (no state!)</span>
    <span class="keyword">return</span> action_time_emb</code></pre>

        <h5>Step 4: Flow Matching Forward Pass</h5>
        <pre><code><span class="keyword">def</span> <span class="function">forward</span>(<span class="keyword">self</span>, batch, noise=<span class="keyword">None</span>, time=<span class="keyword">None</span>):
    <span class="comment"># Prepare inputs</span>
    images, img_masks = <span class="keyword">self</span>.prepare_images(batch)
    state = <span class="keyword">self</span>.prepare_state(batch)
    actions = <span class="keyword">self</span>.prepare_action(batch)  <span class="comment"># (B, chunk_size, max_action_dim)</span>
    lang_tokens = batch[<span class="string">"observation.language_tokens"</span>]
    
    <span class="comment"># Sample noise and time for flow matching</span>
    <span class="keyword">if</span> noise <span class="keyword">is None</span>:
        noise = <span class="keyword">self</span>.sample_noise(actions.shape, actions.device)  <span class="comment"># N(0, 1)</span>
    <span class="keyword">if</span> time <span class="keyword">is None</span>:
        time = <span class="keyword">self</span>.sample_time(batch_size, device)  <span class="comment"># Beta distribution</span>
    
    <span class="comment"># Flow matching interpolation: x_t = t * noise + (1-t) * actions</span>
    time_expanded = time[:, <span class="keyword">None</span>, <span class="keyword">None</span>]  <span class="comment"># (B, 1, 1)</span>
    x_t = time_expanded * noise + (<span class="number">1</span> - time_expanded) * actions  <span class="comment"># Noisy actions</span>
    u_t = noise - actions  <span class="comment"># Target velocity field</span>
    
    <span class="comment"># Embed prefix and suffix</span>
    prefix_embs, prefix_masks, prefix_att = <span class="keyword">self</span>.embed_prefix(images, img_masks, lang_tokens, lang_masks, state)
    suffix_embs, suffix_masks, suffix_att = <span class="keyword">self</span>.embed_suffix(x_t, time)
    
    <span class="comment"># Forward through VLM + Expert</span>
    pred_velocity = <span class="keyword">self</span>.vlm_with_expert(prefix_embs, suffix_embs, ...)
    
    <span class="comment"># Compute loss: MSE between predicted and target velocity</span>
    loss = F.mse_loss(pred_velocity, u_t)  <span class="comment"># Flow matching loss</span>
    
    <span class="keyword">return</span> loss, loss_dict</code></pre>

        <div class="key-concept">
            <h4>ğŸ”‘ Key Concept: Flow Matching</h4>
            <p>Flow Matching is a generative technique that learns to map noise to data by predicting velocity fields:</p>
            <ul>
                <li><strong>Interpolation:</strong> <code>x_t = t Ã— noise + (1-t) Ã— action</code> (t âˆˆ [0, 1])</li>
                <li><strong>Target velocity:</strong> <code>u_t = noise - action</code> (straight line from action to noise)</li>
                <li><strong>Training:</strong> Network predicts <code>v_Î¸(x_t, t)</code>, minimize <code>||v_Î¸ - u_t||Â²</code></li>
                <li><strong>Inference:</strong> Integrate from noise using Euler: <code>x_{t+dt} = x_t + dt Ã— v_Î¸(x_t, t)</code></li>
            </ul>
        </div>

        <h4>SmolVLA Preprocessing Pipeline</h4>
        <pre><code><span class="comment"># From processor_smolvla.py</span>
input_steps = [
    RenameObservationsProcessorStep(rename_map={}),
    AddBatchDimensionProcessorStep(),
    SmolVLANewLineProcessor(),  <span class="comment"># Ensure task ends with \n</span>
    TokenizerProcessorStep(
        tokenizer_name=<span class="string">"HuggingFaceTB/SmolVLM2-500M-Video-Instruct"</span>,
        max_length=<span class="number">48</span>,
        padding=<span class="string">"max_length"</span>,
    ),
    DeviceProcessorStep(device=<span class="string">"cuda"</span>),
    NormalizerProcessorStep(
        features={**input_features, **output_features},
        norm_map={<span class="string">"STATE"</span>: <span class="string">"mean_std"</span>, <span class="string">"ACTION"</span>: <span class="string">"mean_std"</span>, <span class="string">"VISUAL"</span>: <span class="string">"identity"</span>},
        stats=dataset_stats,
    ),
]</code></pre>
    </div>

    <!-- ============================================ -->
    <!-- SECTION 5: Pi0 -->
    <!-- ============================================ -->
    <h2 id="pi0">ğŸ¥§ 5. Pi0: PaliGemma + Expert Model</h2>

    <div class="model-section">
        <div class="model-header">
            <span class="model-icon">ğŸ¯</span>
            <div>
                <h3 style="margin: 0; color: var(--accent-orange);">Pi0 Architecture</h3>
                <p style="margin: 5px 0 0 0; color: var(--text-secondary);">PaliGemma (SigLIP + Gemma) backbone with action expert</p>
            </div>
        </div>

        <div class="architecture-box">
            <div class="arch-diagram">
                <!-- Input Layer -->
                <div class="arch-row" style="justify-content: space-around;">
                    <div class="arch-box blue">
                        <div class="arch-box-title">Images</div>
                        <div class="arch-box-shape">(B, num_cams, C, H, W)</div>
                        <div class="arch-box-desc">224Ã—224</div>
                    </div>
                    <div class="arch-box green">
                        <div class="arch-box-title">Language</div>
                        <div class="arch-box-shape">(B, seq_len)</div>
                    </div>
                </div>

                <!-- Arrow Down -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-arrow">â†“</div>
                </div>

                <!-- Encoding Layer -->
                <div class="arch-row" style="justify-content: space-around;">
                    <div class="arch-box blue">
                        <div class="arch-box-title">SigLIP Vision Tower</div>
                        <div class="arch-box-desc">Extract visual features</div>
                    </div>
                    <div class="arch-box green">
                        <div class="arch-box-title">PaliGemma Tokenizer</div>
                        <div class="arch-box-desc">Tokenize language</div>
                    </div>
                </div>

                <!-- Arrow Down -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-arrow">â†“</div>
                </div>

                <!-- Projection Layer -->
                <div class="arch-row" style="justify-content: space-around;">
                    <div class="arch-box blue">
                        <div class="arch-box-title">Multi-modal Projector</div>
                        <div class="arch-box-desc">Connect vision to language</div>
                    </div>
                    <div class="arch-box green">
                        <div class="arch-box-title">Gemma LLM Embedding</div>
                        <div class="arch-box-desc">Language embeddings</div>
                    </div>
                </div>

                <!-- Merge Arrow -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-merge">
                        <div class="arch-merge-line"></div>
                        <div class="arch-arrow">â†“</div>
                    </div>
                </div>

                <!-- Prefix Section -->
                <div class="arch-section">
                    <div class="arch-section-title">PREFIX: [imgâ‚, imgâ‚‚, ..., langâ‚, langâ‚‚, ...]</div>
                </div>

                <!-- Arrow Down -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-arrow">â†“</div>
                </div>

                <!-- PaliGemma Language Model -->
                <div class="arch-box blue" style="max-width: 100%;">
                    <div class="arch-box-title">PaliGemma Language Model (Frozen)</div>
                    <div class="arch-box-desc">â€¢ Processes prefix embeddings</div>
                    <div class="arch-box-desc">â€¢ Outputs hidden states for action expert</div>
                </div>

                <!-- Arrow Down with branches -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-arrow">â†“</div>
                </div>

                <!-- Suffix Inputs -->
                <div class="arch-row" style="justify-content: space-around;">
                    <div class="arch-box orange">
                        <div class="arch-box-title">State Projection</div>
                        <div class="arch-box-desc">Linear(max_state_dim â†’ hidden)</div>
                    </div>
                    <div class="arch-box orange">
                        <div class="arch-box-title">Noisy Actions</div>
                    </div>
                    <div class="arch-box orange">
                        <div class="arch-box-title">Timestep Embedding</div>
                    </div>
                </div>

                <!-- Merge Arrow -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-merge">
                        <div class="arch-merge-line"></div>
                        <div class="arch-arrow">â†“</div>
                    </div>
                </div>

                <!-- Suffix Section -->
                <div class="arch-section">
                    <div class="arch-section-title">SUFFIX: [state_emb, noisy_actâ‚, ..., noisy_actâ‚™]</div>
                    <div style="margin-top: 8px; font-size: 0.85rem; color: var(--text-secondary); text-align: center;">
                        Note: Pi0 uses robot state via state_proj layer
                    </div>
                </div>

                <!-- Arrow Down -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-arrow">â†“</div>
                </div>

                <!-- Gemma Expert -->
                <div class="arch-box orange" style="max-width: 100%;">
                    <div class="arch-box-title">Gemma Expert (Trainable)</div>
                    <div class="arch-box-desc">â€¢ Attends to both prefix (VLM) and suffix (state/actions)</div>
                    <div class="arch-box-desc">â€¢ Joint attention mechanism</div>
                    <div class="arch-box-desc">â€¢ Predicts velocity for flow matching</div>
                </div>

                <!-- Arrow Down -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-arrow">â†“</div>
                </div>

                <!-- Output -->
                <div class="arch-box green" style="max-width: 100%;">
                    <div class="arch-box-title">Predicted Action Chunk</div>
                    <div class="arch-box-shape">Shape: (B, chunk_size, action_dim)</div>
                </div>
            </div>
        </div>

        <h4>Key Differences from SmolVLA</h4>
        <ul>
            <li><strong>VLM Backbone:</strong> PaliGemma (Gemma 2B + SigLIP) vs SmolVLM2</li>
            <li><strong>Image Resolution:</strong> 224Ã—224 (default) vs 512Ã—512</li>
            <li><strong>Expert Architecture:</strong> Gemma 300M decoder vs lightweight expert</li>
            <li><strong>Joint Attention:</strong> Expert attends to both VLM output and its own tokens</li>
            <li><strong>Robot State Position:</strong> Pi0 embeds state in <strong>SUFFIX</strong>; SmolVLA embeds state in <strong>PREFIX</strong></li>
        </ul>

        <div class="warning-box">
            <strong>âš ï¸ Critical: State Handling Across Models</strong>
            <ul style="margin: 10px 0 0 20px;">
                <li><strong>SmolVLA:</strong> State embedded via <code>state_proj</code> in <strong>PREFIX</strong>: <code>[images, language, state_emb]</code></li>
                <li><strong>Pi0:</strong> State embedded via <code>state_proj</code> in <strong>SUFFIX</strong>: <code>[state_emb, noisy_actions+timestep]</code></li>
                <li><strong>Pi0.5:</strong> State is <strong>discretized into 256 bins and tokenized into the language prompt</strong> as text: <code>"Task: {task}, State: 128 130 127...;\nAction: "</code>. No separate state embedding layer. Uses AdaRMS for timestep conditioning.</li>
            </ul>
        </div>

        <h4>Pi0 Forward Pass</h4>
        <pre><code><span class="keyword">def</span> <span class="function">forward</span>(<span class="keyword">self</span>, images, img_masks, lang_tokens, lang_masks, state, actions, noise=<span class="keyword">None</span>, time=<span class="keyword">None</span>):
    <span class="comment"># Sample noise and time</span>
    <span class="keyword">if</span> noise <span class="keyword">is None</span>:
        noise = <span class="keyword">self</span>.sample_noise(actions.shape, actions.device)
    <span class="keyword">if</span> time <span class="keyword">is None</span>:
        time = <span class="keyword">self</span>.sample_time(batch_size, device)
    
    <span class="comment"># Flow matching interpolation</span>
    time_expanded = time[:, <span class="keyword">None</span>, <span class="keyword">None</span>]
    x_t = time_expanded * noise + (<span class="number">1</span> - time_expanded) * actions
    u_t = noise - actions
    
    <span class="comment"># Embed prefix (images + language)</span>
    prefix_embs, prefix_pad_masks, prefix_att_masks = <span class="keyword">self</span>.embed_prefix(
        images, img_masks, lang_tokens, lang_masks
    )
    
    <span class="comment"># Embed suffix (state + noisy actions + timestep)</span>
    <span class="comment"># Note: Pi0 includes state, but Pi0.5 does NOT use state</span>
    <span class="comment"># Pi0: embed_suffix(state, x_t, time) - includes state</span>
    <span class="comment"># Pi0.5: embed_suffix(x_t, time) - NO state parameter</span>
    suffix_embs, suffix_pad_masks, suffix_att_masks, adarms_cond = <span class="keyword">self</span>.embed_suffix(
        state, x_t, time
    )
    
    <span class="comment"># Combine prefix and suffix</span>
    pad_masks = torch.cat([prefix_pad_masks, suffix_pad_masks], dim=<span class="number">1</span>)
    att_masks = torch.cat([prefix_att_masks, suffix_att_masks], dim=<span class="number">1</span>)
    
    <span class="comment"># Create 2D attention mask</span>
    att_2d_masks = make_att_2d_masks(pad_masks, att_masks)
    
    <span class="comment"># Joint forward through PaliGemma + Expert</span>
    prefix_output, suffix_output = <span class="keyword">self</span>.paligemma_with_expert(
        inputs_embeds=[prefix_embs, suffix_embs],
        attention_mask=att_2d_masks,
        ...
    )
    
    <span class="comment"># Project to action space</span>
    pred_velocity = <span class="keyword">self</span>.action_out_proj(suffix_output[:, -chunk_size:])
    
    <span class="comment"># Flow matching loss</span>
    loss = F.mse_loss(pred_velocity, u_t)
    
    <span class="keyword">return</span> loss</code></pre>
    </div>

    <!-- ============================================ -->
    <!-- SECTION 6: Pi0.5 -->
    <!-- ============================================ -->
    <h2 id="pi05">ğŸš€ 6. Pi0.5: No State, AdaRMS Conditioning</h2>

    <div class="model-section">
        <div class="model-header">
            <span class="model-icon">âš¡</span>
            <div>
                <h3 style="margin: 0; color: var(--accent-purple);">Pi0.5 Architecture</h3>
                <p style="margin: 5px 0 0 0; color: var(--text-secondary);">PaliGemma backbone with AdaRMS conditioning, NO robot state</p>
            </div>
        </div>

        <div class="architecture-box">
            <div class="arch-diagram">
                <!-- Input Layer -->
                <div class="arch-row" style="justify-content: space-around;">
                    <div class="arch-box blue">
                        <div class="arch-box-title">Images</div>
                        <div class="arch-box-shape">(B, num_cams, C, H, W)</div>
                        <div class="arch-box-desc">224Ã—224</div>
                    </div>
                    <div class="arch-box green">
                        <div class="arch-box-title">Language</div>
                        <div class="arch-box-shape">(B, seq_len)</div>
                        <div class="arch-box-desc">200 tokens (vs 48 in Pi0)</div>
                    </div>
                </div>

                <!-- Arrow Down -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-arrow">â†“</div>
                </div>

                <!-- Encoding Layer -->
                <div class="arch-row" style="justify-content: space-around;">
                    <div class="arch-box blue">
                        <div class="arch-box-title">SigLIP Vision Tower</div>
                        <div class="arch-box-desc">Extract visual features</div>
                    </div>
                    <div class="arch-box green">
                        <div class="arch-box-title">PaliGemma Tokenizer</div>
                        <div class="arch-box-desc">Tokenize language</div>
                    </div>
                </div>

                <!-- Arrow Down -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-arrow">â†“</div>
                </div>

                <!-- Projection Layer -->
                <div class="arch-row" style="justify-content: space-around;">
                    <div class="arch-box blue">
                        <div class="arch-box-title">Multi-modal Projector</div>
                        <div class="arch-box-desc">Connect vision to language</div>
                    </div>
                    <div class="arch-box green">
                        <div class="arch-box-title">Gemma LLM Embedding</div>
                        <div class="arch-box-desc">Language embeddings</div>
                    </div>
                </div>

                <!-- Merge Arrow -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-merge">
                        <div class="arch-merge-line"></div>
                        <div class="arch-arrow">â†“</div>
                    </div>
                </div>

                <!-- Prefix Section -->
                <div class="arch-section">
                    <div class="arch-section-title">PREFIX: [imgâ‚, imgâ‚‚, ..., langâ‚, langâ‚‚, ...]</div>
                </div>

                <!-- Arrow Down -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-arrow">â†“</div>
                </div>

                <!-- PaliGemma Language Model -->
                <div class="arch-box blue" style="max-width: 100%;">
                    <div class="arch-box-title">PaliGemma Language Model (Frozen)</div>
                    <div class="arch-box-desc">â€¢ Processes prefix embeddings</div>
                    <div class="arch-box-desc">â€¢ Outputs hidden states for action expert</div>
                </div>

                <!-- Arrow Down -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-arrow">â†“</div>
                </div>

                <!-- Suffix Inputs (NO STATE) -->
                <div class="arch-row" style="justify-content: space-around;">
                    <div class="arch-box orange">
                        <div class="arch-box-title">Noisy Actions</div>
                    </div>
                    <div class="arch-box purple">
                        <div class="arch-box-title">Timestep Embedding</div>
                        <div class="arch-box-desc">â†’ AdaRMS conditioning</div>
                    </div>
                </div>

                <!-- Merge Arrow -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-merge">
                        <div class="arch-merge-line"></div>
                        <div class="arch-arrow">â†“</div>
                    </div>
                </div>

                <!-- Suffix Section -->
                <div class="arch-section">
                    <div class="arch-section-title">SUFFIX: [noisy_actâ‚, ..., noisy_actâ‚™]</div>
                    <div style="margin-top: 8px; font-size: 0.85rem; color: var(--accent-red); text-align: center; font-weight: 600;">
                        âš ï¸ NO Robot State - Uses AdaRMS conditioning instead
                    </div>
                </div>

                <!-- Arrow Down -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-arrow">â†“</div>
                </div>

                <!-- Gemma Expert with AdaRMS -->
                <div class="arch-box purple" style="max-width: 100%;">
                    <div class="arch-box-title">Gemma Expert (Trainable) with AdaRMS</div>
                    <div class="arch-box-desc">â€¢ Attends to both prefix (VLM) and suffix (actions)</div>
                    <div class="arch-box-desc">â€¢ AdaRMS conditioning via timestep embedding</div>
                    <div class="arch-box-desc">â€¢ Predicts velocity for flow matching</div>
                    <div class="arch-box-desc">â€¢ Smaller expert: gemma_300m (vs gemma_2b in Pi0)</div>
                </div>

                <!-- Arrow Down -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-arrow">â†“</div>
                </div>

                <!-- Output -->
                <div class="arch-box green" style="max-width: 100%;">
                    <div class="arch-box-title">Predicted Action Chunk</div>
                    <div class="arch-box-shape">Shape: (B, chunk_size, action_dim)</div>
                </div>
            </div>
        </div>

        <h4>Key Differences from Pi0</h4>
        <ul>
            <li><strong>Robot State:</strong> Pi0.5 <strong>discretizes state into 256 bins and puts it in the language prompt</strong> as text: <code>"Task: {task}, State: 128 130 127...;\nAction: "</code>. No separate <code>state_proj</code> embedding layer.</li>
            <li><strong>AdaRMS Conditioning:</strong> Uses Adaptive RMSNorm (AdaRMS) for timestep conditioning instead of concatenating timestep with action embeddings</li>
            <li><strong>Tokenizer Length:</strong> 200 tokens (vs 48 in Pi0) to accommodate the discretized state string in the prompt</li>
            <li><strong>Expert Size:</strong> Same as Pi0 (gemma_300m default for both)</li>
            <li><strong>Normalization:</strong> Uses QUANTILES normalization for state/action (vs MEAN_STD in Pi0) - state is normalized to [-1, 1] before discretization</li>
            <li><strong>Time Conditioning:</strong> Uses <code>time_mlp_*</code> for AdaRMS conditioning (vs <code>action_time_mlp_*</code> in Pi0)</li>
        </ul>
        
        <h4>ğŸ”‘ Pi0.5 State Discretization in Language Prompt</h4>
        <pre><code><span class="comment"># From Pi05PrepareStateTokenizerProcessorStep</span>
<span class="comment"># 1. State is normalized to [-1, 1] by NormalizerProcessorStep (QUANTILES mode)</span>
<span class="comment"># 2. Discretize into 256 bins</span>
discretized_states = np.digitize(state_np, bins=np.linspace(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">256</span> + <span class="number">1</span>)[:-<span class="number">1</span>]) - <span class="number">1</span>
<span class="comment"># 3. Convert to string and append to prompt</span>
state_str = <span class="string">" "</span>.join(map(str, discretized_states[i]))
full_prompt = f<span class="string">"Task: {cleaned_text}, State: {state_str};\nAction: "</span>
<span class="comment"># Example: "Task: pick up the cube, State: 128 130 127 125 129 128;\nAction: "</span></code></pre>

        <h4>Pi0.5 Forward Pass</h4>
        <pre><code><span class="keyword">def</span> <span class="function">forward</span>(<span class="keyword">self</span>, images, img_masks, tokens, masks, actions, noise=<span class="keyword">None</span>, time=<span class="keyword">None</span>):
    <span class="comment"># Note: NO state parameter!</span>
    <span class="comment"># Sample noise and time</span>
    <span class="keyword">if</span> noise <span class="keyword">is None</span>:
        noise = <span class="keyword">self</span>.sample_noise(actions.shape, actions.device)
    <span class="keyword">if</span> time <span class="keyword">is None</span>:
        time = <span class="keyword">self</span>.sample_time(batch_size, device)
    
    <span class="comment"># Flow matching interpolation</span>
    time_expanded = time[:, <span class="keyword">None</span>, <span class="keyword">None</span>]
    x_t = time_expanded * noise + (<span class="number">1</span> - time_expanded) * actions
    u_t = noise - actions
    
    <span class="comment"># Embed prefix (images + language)</span>
    prefix_embs, prefix_pad_masks, prefix_att_masks = <span class="keyword">self</span>.embed_prefix(
        images, img_masks, tokens, masks
    )
    
    <span class="comment"># Embed suffix (NO state - only noisy actions + timestep)</span>
    suffix_embs, suffix_pad_masks, suffix_att_masks, adarms_cond = <span class="keyword">self</span>.embed_suffix(
        x_t, time  <span class="comment"># NO state parameter!</span>
    )
    
    <span class="comment"># Combine prefix and suffix</span>
    pad_masks = torch.cat([prefix_pad_masks, suffix_pad_masks], dim=<span class="number">1</span>)
    att_masks = torch.cat([prefix_att_masks, suffix_att_masks], dim=<span class="number">1</span>)
    
    <span class="comment"># Joint forward through PaliGemma + Expert with AdaRMS</span>
    prefix_output, suffix_output = <span class="keyword">self</span>.paligemma_with_expert(
        inputs_embeds=[prefix_embs, suffix_embs],
        attention_mask=att_2d_masks,
        adarms_cond=[<span class="keyword">None</span>, adarms_cond],  <span class="comment"># AdaRMS conditioning</span>
        ...
    )
    
    <span class="comment"># Project to action space</span>
    pred_velocity = <span class="keyword">self</span>.action_out_proj(suffix_output[:, -chunk_size:])
    
    <span class="comment"># Flow matching loss</span>
    loss = F.mse_loss(pred_velocity, u_t)
    
    <span class="keyword">return</span> loss</code></pre>

        <h4>Pi0.5 Embed Suffix (No State)</h4>
        <pre><code><span class="keyword">def</span> <span class="function">embed_suffix</span>(<span class="keyword">self</span>, noisy_actions, timestep):
    <span class="string">"""Embed noisy_actions, timestep - NO state parameter!"""</span>
    embs = []
    
    <span class="comment"># Embed timestep using sine-cosine positional encoding</span>
    time_emb = create_sinusoidal_pos_embedding(timestep, ...)
    
    <span class="comment"># Process timestep through MLP for AdaRMS conditioning</span>
    time_emb = <span class="keyword">self</span>.time_mlp_in(time_emb)
    time_emb = F.silu(time_emb)
    time_emb = <span class="keyword">self</span>.time_mlp_out(time_emb)
    time_emb = F.silu(time_emb)
    
    <span class="comment"># Project noisy actions</span>
    action_emb = <span class="keyword">self</span>.action_in_proj(noisy_actions)
    
    <span class="comment"># AdaRMS conditioning (not concatenated with actions)</span>
    adarms_cond = time_emb
    
    embs.append(action_emb)
    
    <span class="keyword">return</span> embs, pad_masks, att_masks, adarms_cond</code></pre>

        <div class="key-concept">
            <h4>ğŸ”‘ Key Concept: AdaRMS Conditioning</h4>
            <p>Pi0.5 uses Adaptive RMSNorm (AdaRMS) for timestep conditioning:</p>
            <ul>
                <li><strong>Pi0:</strong> Concatenates timestep embedding with action embeddings: <code>[action_emb, time_emb] â†’ MLP</code></li>
                <li><strong>Pi0.5:</strong> Uses timestep embedding as AdaRMS condition: <code>adarms_cond = time_emb</code>, passed separately to expert layers</li>
                <li><strong>Benefit:</strong> More efficient conditioning, allows model to adapt normalization based on timestep</li>
            </ul>
        </div>
    </div>

    <!-- ============================================ -->
    <!-- SECTION 7: Pi0.5-Fast -->
    <!-- ============================================ -->
    <h2 id="pi05-fast">âš¡ 7. Pi0.5-Fast: Optimized for Speed</h2>

    <div class="model-section">
        <div class="model-header">
            <span class="model-icon">ğŸƒ</span>
            <div>
                <h3 style="margin: 0; color: var(--accent-yellow);">Pi0.5-Fast Architecture</h3>
                <p style="margin: 5px 0 0 0; color: var(--text-secondary);">Pi0.5 optimized for faster inference with reduced inference steps</p>
            </div>
        </div>

        <div class="info-box">
            <strong>ğŸ“Œ Pi0.5-Fast Configuration</strong>
            <p>Pi0.5-Fast is the same architecture as Pi0.5, but optimized for faster inference:</p>
            <ul>
                <li><strong>Fewer Inference Steps:</strong> Uses <code>num_inference_steps=5</code> (vs 10 in standard Pi0.5)</li>
                <li><strong>Same Architecture:</strong> Identical to Pi0.5 (no state, AdaRMS conditioning)</li>
                <li><strong>Trade-off:</strong> Slightly lower action quality for ~2x faster inference</li>
                <li><strong>Use Case:</strong> Real-time robot control where speed is critical</li>
            </ul>
        </div>

        <h4>Configuration Differences</h4>
        <table class="comparison-table">
            <thead>
                <tr>
                    <th>Parameter</th>
                    <th>Pi0.5</th>
                    <th>Pi0.5-Fast</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><code>num_inference_steps</code></td>
                    <td>10</td>
                    <td>5</td>
                </tr>
                <tr>
                    <td><code>action_expert_variant</code></td>
                    <td>gemma_300m</td>
                    <td>gemma_300m</td>
                </tr>
                <tr>
                    <td><code>paligemma_variant</code></td>
                    <td>gemma_2b</td>
                    <td>gemma_2b</td>
                </tr>
                <tr>
                    <td><code>chunk_size</code></td>
                    <td>50</td>
                    <td>50</td>
                </tr>
                <tr>
                    <td><strong>Inference Speed</strong></td>
                    <td>Baseline</td>
                    <td>~2x faster</td>
                </tr>
            </tbody>
        </table>

        <h4>Usage Example</h4>
        <pre><code><span class="comment"># Standard Pi0.5</span>
lerobot-train \
    --policy.path=lerobot/pi05_base \
    --policy.num_inference_steps=10 \
    ...

<span class="comment"># Pi0.5-Fast (faster inference)</span>
lerobot-train \
    --policy.path=lerobot/pi05_base \
    --policy.num_inference_steps=5 \
    ...</code></pre>

        <div class="warning-box">
            <strong>âš ï¸ Note on Pi0.5-Fast</strong>
            <p>Pi0.5-Fast is not a separate model variant, but rather Pi0.5 configured with fewer inference steps. The architecture is identical to Pi0.5. You can achieve the same result by setting <code>--policy.num_inference_steps=5</code> when using Pi0.5.</p>
        </div>
    </div>

    <!-- ============================================ -->
    <!-- SECTION 8: ACT -->
    <!-- ============================================ -->
    <h2 id="act">ğŸ¬ 8. ACT: Action Chunking Transformer</h2>

    <div class="model-section">
        <div class="model-header">
            <span class="model-icon">âš¡</span>
            <div>
                <h3 style="margin: 0; color: var(--accent-green);">ACT Architecture</h3>
                <p style="margin: 5px 0 0 0; color: var(--text-secondary);">Encoder-Decoder Transformer with optional VAE</p>
            </div>
        </div>

        <div class="architecture-box">
            <div class="arch-diagram">
                <!-- Training Mode Label -->
                <div class="arch-section" style="border-color: var(--accent-purple); margin-bottom: 20px;">
                    <div class="arch-section-title">Training Mode (with VAE)</div>
                </div>

                <!-- Input Layer -->
                <div class="arch-row" style="justify-content: space-around;">
                    <div class="arch-box orange">
                        <div class="arch-box-title">Actions</div>
                        <div class="arch-box-shape">(B, chunk_size, action_dim)</div>
                    </div>
                    <div class="arch-box blue">
                        <div class="arch-box-title">Robot State</div>
                        <div class="arch-box-shape">(B, state_dim)</div>
                    </div>
                </div>

                <!-- Arrow Down -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-arrow">â†“</div>
                </div>

                <!-- VAE Encoder -->
                <div class="arch-box orange" style="max-width: 100%;">
                    <div class="arch-box-title">VAE Encoder (BERT-style)</div>
                    <div class="arch-box-desc">Input: [CLS] + action_embedâ‚ + ... + action_embedâ‚™ + state</div>
                    <div class="arch-box-desc">Positional embedding + Self-attention</div>
                    <div class="arch-box-desc">Output: CLS token â†’ Î¼, log(ÏƒÂ²)</div>
                </div>

                <!-- Arrow Down -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-arrow">â†“</div>
                </div>

                <!-- Reparameterization -->
                <div class="arch-box blue" style="max-width: 100%;">
                    <div class="arch-box-title">Reparameterization</div>
                    <div class="arch-box-desc">z = Î¼ + Ïƒ Ã— Îµ, Îµ ~ N(0,1)</div>
                </div>

                <!-- Arrow Down -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-arrow">â†“</div>
                    <div style="margin-left: 10px; color: var(--text-secondary); font-size: 0.85rem;">Latent z</div>
                </div>

                <!-- Encoder Inputs -->
                <div class="arch-row" style="justify-content: space-around;">
                    <div class="arch-box green">
                        <div class="arch-box-title">Latent Projection</div>
                        <div class="arch-box-desc">z â†’ tokens</div>
                    </div>
                    <div class="arch-box blue">
                        <div class="arch-box-title">State Projection</div>
                        <div class="arch-box-desc">state â†’ tokens</div>
                    </div>
                    <div class="arch-box purple">
                        <div class="arch-box-title">Camera Features</div>
                        <div class="arch-box-desc">ResNet18 â†’ Flatten â†’ Project</div>
                    </div>
                </div>

                <!-- Merge Arrow -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-merge">
                        <div class="arch-merge-line"></div>
                        <div class="arch-arrow">â†“</div>
                    </div>
                </div>

                <!-- Transformer Encoder -->
                <div class="arch-box green" style="max-width: 100%;">
                    <div class="arch-box-title">TRANSFORMER ENCODER</div>
                    <div class="arch-box-desc">Input tokens: [latent_proj(z), state_proj(state), cam_features]</div>
                    <div class="arch-box-desc">Encoder attention layers</div>
                </div>

                <!-- Arrow Down -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-arrow">â†“</div>
                    <div style="margin-left: 10px; color: var(--text-secondary); font-size: 0.85rem;">Encoder output</div>
                </div>

                <!-- Transformer Decoder -->
                <div class="arch-box blue" style="max-width: 100%;">
                    <div class="arch-box-title">TRANSFORMER DECODER</div>
                    <div class="arch-box-desc">Query tokens: [chunk_size] learnable position embeddings</div>
                    <div class="arch-box-desc">Cross-attention to encoder output</div>
                    <div class="arch-box-desc">Self-attention between query tokens</div>
                </div>

                <!-- Arrow Down -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-arrow">â†“</div>
                </div>

                <!-- Action Head -->
                <div class="arch-box orange" style="max-width: 100%;">
                    <div class="arch-box-title">Action Head (Linear)</div>
                    <div class="arch-box-shape">Input: (B, chunk_size, dim_model)</div>
                    <div class="arch-box-shape">Output: (B, chunk_size, action_dim)</div>
                </div>

                <!-- Loss -->
                <div class="arch-section" style="border-color: var(--accent-pink); margin-top: 20px;">
                    <div class="arch-section-title">Loss = L1(predicted, target) + Î² Ã— KL(q(z|x) || p(z))</div>
                </div>
            </div>
        </div>

        <h4>ACT Key Features</h4>
        <ul>
            <li><strong>Vision Backbone:</strong> ResNet18 (pretrained on ImageNet) for image encoding</li>
            <li><strong>VAE Objective:</strong> Optional variational objective for diverse action generation</li>
            <li><strong>Action Chunking:</strong> Predicts entire action sequence (100 steps) at once</li>
            <li><strong>Temporal Ensembling:</strong> Optional exponential weighted averaging of predictions</li>
        </ul>

        <h4>ACT Forward Pass</h4>
        <pre><code><span class="keyword">def</span> <span class="function">forward</span>(<span class="keyword">self</span>, batch):
    batch_size = batch[<span class="string">"observation.state"</span>].shape[<span class="number">0</span>]
    
    <span class="comment"># 1. VAE Encoder (if use_vae=True)</span>
    <span class="keyword">if</span> <span class="keyword">self</span>.config.use_vae:
        <span class="comment"># Create VAE input: [CLS, action_embed, state_embed]</span>
        cls_token = <span class="keyword">self</span>.cls_embed.weight.unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, batch_size, <span class="number">1</span>)
        action_embed = <span class="keyword">self</span>.vae_encoder_action_input_proj(batch[<span class="string">"action"</span>])
        robot_state_embed = <span class="keyword">self</span>.vae_encoder_robot_state_input_proj(batch[<span class="string">"observation.state"</span>])
        
        vae_encoder_input = torch.cat([cls_token, action_embed, robot_state_embed], dim=<span class="number">0</span>)
        
        <span class="comment"># Forward through VAE encoder</span>
        cls_token_out = <span class="keyword">self</span>.vae_encoder(vae_encoder_input)
        latent_params = <span class="keyword">self</span>.vae_encoder_latent_output_proj(cls_token_out)
        
        mu = latent_params[:, :latent_dim]
        log_sigma_x2 = latent_params[:, latent_dim:]
        
        <span class="comment"># Reparameterization trick</span>
        latent_sample = mu + log_sigma_x2.div(<span class="number">2</span>).exp() * torch.randn_like(mu)
    <span class="keyword">else</span>:
        latent_sample = torch.zeros([batch_size, latent_dim])
    
    <span class="comment"># 2. Prepare encoder inputs</span>
    encoder_in_tokens = [<span class="keyword">self</span>.encoder_latent_input_proj(latent_sample)]
    encoder_in_tokens.append(<span class="keyword">self</span>.encoder_robot_state_input_proj(batch[<span class="string">"observation.state"</span>]))
    
    <span class="comment"># 3. Process camera images with ResNet backbone</span>
    <span class="keyword">for</span> img <span class="keyword">in</span> batch[<span class="string">"observation.images"</span>]:
        cam_features = <span class="keyword">self</span>.backbone(img)[<span class="string">"feature_map"</span>]  <span class="comment"># (B, 512, H', W')</span>
        cam_features = <span class="keyword">self</span>.encoder_img_feat_input_proj(cam_features)  <span class="comment"># â†’ (B, dim_model, H', W')</span>
        cam_features = einops.rearrange(cam_features, <span class="string">"b c h w -> (h w) b c"</span>)  <span class="comment"># Flatten spatial</span>
        encoder_in_tokens.extend(cam_features)
    
    <span class="comment"># 4. Stack and pass through encoder</span>
    encoder_in_tokens = torch.stack(encoder_in_tokens, axis=<span class="number">0</span>)
    encoder_out = <span class="keyword">self</span>.encoder(encoder_in_tokens)
    
    <span class="comment"># 5. Decoder: query tokens cross-attend to encoder output</span>
    decoder_in = torch.zeros((chunk_size, batch_size, dim_model))
    decoder_out = <span class="keyword">self</span>.decoder(decoder_in, encoder_out)
    
    <span class="comment"># 6. Project to action space</span>
    actions = <span class="keyword">self</span>.action_head(decoder_out.transpose(<span class="number">0</span>, <span class="number">1</span>))  <span class="comment"># (B, chunk_size, action_dim)</span>
    
    <span class="keyword">return</span> actions, (mu, log_sigma_x2)</code></pre>

        <h4>ACT Loss Computation</h4>
        <pre><code><span class="comment"># L1 reconstruction loss</span>
l1_loss = F.l1_loss(batch[<span class="string">"action"</span>], actions_hat, reduction=<span class="string">"none"</span>)
l1_loss = (l1_loss * ~batch[<span class="string">"action_is_pad"</span>].unsqueeze(-<span class="number">1</span>)).mean()

<span class="comment"># KL divergence (if VAE)</span>
<span class="keyword">if</span> <span class="keyword">self</span>.config.use_vae:
    mean_kld = (-<span class="number">0.5</span> * (<span class="number">1</span> + log_sigma_x2 - mu.pow(<span class="number">2</span>) - log_sigma_x2.exp())).sum(-<span class="number">1</span>).mean()
    loss = l1_loss + <span class="keyword">self</span>.config.kl_weight * mean_kld
<span class="keyword">else</span>:
    loss = l1_loss</code></pre>
    </div>

    <!-- ============================================ -->
    <!-- SECTION 9: Groot -->
    <!-- ============================================ -->
    <h2 id="groot">ğŸ¤– 9. Groot: NVIDIA GR00T Integration</h2>

    <div class="model-section">
        <div class="model-header">
            <span class="model-icon">ğŸŒ³</span>
            <div>
                <h3 style="margin: 0; color: var(--accent-yellow);">Groot Architecture</h3>
                <p style="margin: 5px 0 0 0; color: var(--text-secondary);">NVIDIA Isaac GR00T N1.5 with Eagle Vision Encoder</p>
            </div>
        </div>

        <div class="architecture-box">
            <div class="arch-diagram">
                <!-- Input Layer -->
                <div class="arch-row" style="justify-content: space-around;">
                    <div class="arch-box blue">
                        <div class="arch-box-title">Video Frames</div>
                        <div class="arch-box-shape">(B, T, V, H, W, C)</div>
                    </div>
                    <div class="arch-box green">
                        <div class="arch-box-title">Language Task</div>
                        <div class="arch-box-shape">(B,)</div>
                    </div>
                </div>

                <!-- Arrow Down -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-arrow">â†“</div>
                </div>

                <!-- Eagle Processor -->
                <div class="arch-box blue" style="max-width: 100%;">
                    <div class="arch-box-title">Eagle Vision-Language Processor</div>
                    <div class="arch-box-desc">â€¢ Dynamic tiling for multi-view video</div>
                    <div class="arch-box-desc">â€¢ Chat template formatting</div>
                    <div class="arch-box-desc">â€¢ Returns: eagle_pixel_values, eagle_input_ids, etc.</div>
                </div>

                <!-- Arrow Down -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-arrow">â†“</div>
                </div>

                <!-- Eagle VL Model -->
                <div class="arch-box green" style="max-width: 100%;">
                    <div class="arch-box-title">Eagle Vision-Language Model</div>
                    <div class="arch-box-desc">â€¢ Processes video frames + language</div>
                    <div class="arch-box-desc">â€¢ Outputs: vision-language embeddings</div>
                </div>

                <!-- Arrow Down with branches -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-arrow">â†“</div>
                </div>

                <!-- Encoders -->
                <div class="arch-row" style="justify-content: space-around;">
                    <div class="arch-box orange">
                        <div class="arch-box-title">State Encoder</div>
                    </div>
                    <div class="arch-box orange">
                        <div class="arch-box-title">Action Encoder</div>
                    </div>
                    <div class="arch-box orange">
                        <div class="arch-box-title">Embodiment ID</div>
                    </div>
                </div>

                <!-- Merge Arrow -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-merge">
                        <div class="arch-merge-line"></div>
                        <div class="arch-arrow">â†“</div>
                    </div>
                </div>

                <!-- Flow Matching Action Head -->
                <div class="arch-box orange" style="max-width: 100%;">
                    <div class="arch-box-title">Flow Matching Action Head</div>
                    <div class="arch-box-desc">â€¢ DiT (Diffusion Transformer) architecture</div>
                    <div class="arch-box-desc">â€¢ Cross-attention to VL embeddings</div>
                    <div class="arch-box-desc">â€¢ Predicts action velocity field</div>
                </div>

                <!-- Arrow Down -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-arrow">â†“</div>
                </div>

                <!-- Action Decoder -->
                <div class="arch-box green" style="max-width: 100%;">
                    <div class="arch-box-title">Action Decoder (embodiment-specific)</div>
                    <div class="arch-box-shape">Output: (B, action_horizon, max_action_dim)</div>
                </div>
            </div>
        </div>

        <h4>Groot Preprocessing Pipeline</h4>
        <pre><code><span class="comment"># From processor_groot.py - unique multi-step preprocessing</span>
input_steps = [
    <span class="comment"># 1. Rename keys</span>
    RenameObservationsProcessorStep(rename_map={}),
    
    <span class="comment"># 2. Add batch dimension</span>
    AddBatchDimensionProcessorStep(),
    
    <span class="comment"># 3. Pack inputs: state, action, language, embodiment</span>
    GrootPackInputsStep(
        state_horizon=<span class="number">1</span>,
        action_horizon=<span class="number">16</span>,
        max_state_dim=<span class="number">64</span>,
        max_action_dim=<span class="number">32</span>,
        embodiment_tag=<span class="string">"new_embodiment"</span>,
        normalize_min_max=<span class="keyword">True</span>,  <span class="comment"># Min-max normalize state/action</span>
    ),
    
    <span class="comment"># 4. Eagle encode: convert images to Eagle format</span>
    GrootEagleEncodeStep(
        tokenizer_assets_repo=<span class="string">"nvidia/GR00T-N1.5-3B"</span>,
    ),
    
    <span class="comment"># 5. Collate Eagle content â†’ tensors</span>
    GrootEagleCollateStep(
        tokenizer_assets_repo=<span class="string">"nvidia/GR00T-N1.5-3B"</span>,
    ),
    
    <span class="comment"># 6. Move to device</span>
    DeviceProcessorStep(device=<span class="string">"cuda"</span>),
]</code></pre>

        <h4>Eagle Encoding Process</h4>
        <pre><code><span class="keyword">def</span> <span class="function">__call__</span>(<span class="keyword">self</span>, transition):
    video = obs[<span class="string">"video"</span>]  <span class="comment"># (B, T, V, H, W, C) uint8</span>
    lang = comp.get(<span class="string">"language"</span>, <span class="string">"Perform the task."</span>)
    
    eagle_contents = []
    <span class="keyword">for</span> b <span class="keyword">in</span> range(bsz):
        <span class="comment"># Flatten video frames into list of PIL images</span>
        flat = rearrange(vt, <span class="string">"t v h w c -> (t v) h w c"</span>)
        images = [Image.fromarray(flat[i]) <span class="keyword">for</span> i <span class="keyword">in</span> range(t * v)]
        
        <span class="comment"># Format as chat conversation</span>
        conv = [{
            <span class="string">"role"</span>: <span class="string">"user"</span>,
            <span class="string">"content"</span>: [
                {<span class="string">"type"</span>: <span class="string">"image"</span>, <span class="string">"image"</span>: img} <span class="keyword">for</span> img <span class="keyword">in</span> images
            ] + [{<span class="string">"type"</span>: <span class="string">"text"</span>, <span class="string">"text"</span>: str([lang])}]
        }]
        
        <span class="comment"># Apply chat template</span>
        text_list = [<span class="keyword">self</span>.proc.apply_chat_template(conv, tokenize=<span class="keyword">False</span>)]
        
        eagle_contents.append({
            <span class="string">"text_list"</span>: text_list,
            <span class="string">"image_inputs"</span>: img_inputs,
        })
    
    <span class="keyword">return</span> transition</code></pre>
    </div>

    <!-- ============================================ -->
    <!-- SECTION 10: Diffusion -->
    <!-- ============================================ -->
    <h2 id="diffusion">ğŸŒŠ 10. Diffusion Policy: Denoising Actions</h2>

    <div class="model-section">
        <div class="model-header">
            <span class="model-icon">ğŸ’¨</span>
            <div>
                <h3 style="margin: 0; color: var(--accent-blue);">Diffusion Policy Architecture</h3>
                <p style="margin: 5px 0 0 0; color: var(--text-secondary);">Conditional UNet with DDPM/DDIM noise scheduling</p>
            </div>
        </div>

        <div class="architecture-box">
            <div class="arch-diagram">
                <!-- Input Layer -->
                <div class="arch-row" style="justify-content: space-around;">
                    <div class="arch-box green">
                        <div class="arch-box-title">Images</div>
                        <div class="arch-box-shape">(B, n_obs_steps, num_cams, C, H, W)</div>
                    </div>
                    <div class="arch-box blue">
                        <div class="arch-box-title">State</div>
                        <div class="arch-box-shape">(B, n_obs_steps, D)</div>
                    </div>
                </div>

                <!-- Arrow Down -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-arrow">â†“</div>
                </div>

                <!-- ResNet Encoder -->
                <div class="arch-box green" style="max-width: 100%;">
                    <div class="arch-box-title">ResNet Encoder (per camera)</div>
                    <div class="arch-box-desc">Extract image features</div>
                </div>

                <!-- Arrow Down -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-arrow">â†“</div>
                </div>

                <!-- Global Conditioning -->
                <div class="arch-box purple" style="max-width: 100%;">
                    <div class="arch-box-title">Global Conditioning Vector</div>
                    <div class="arch-box-desc">concat([img_features, state]) â†’ flatten</div>
                    <div class="arch-box-shape">(B, global_cond_dim)</div>
                </div>

                <!-- Arrow Down with branches -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-arrow">â†“</div>
                </div>

                <!-- Training Inputs -->
                <div class="arch-row" style="justify-content: space-around;">
                    <div class="arch-box orange">
                        <div class="arch-box-title">Noise Sample</div>
                        <div class="arch-box-desc">Îµ ~ N(0,1)</div>
                    </div>
                    <div class="arch-box orange">
                        <div class="arch-box-title">Timestep</div>
                        <div class="arch-box-desc">Random t</div>
                    </div>
                    <div class="arch-box orange">
                        <div class="arch-box-title">Actions</div>
                        <div class="arch-box-shape">(B, horizon, action_dim)</div>
                    </div>
                </div>

                <!-- Merge Arrow -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-merge">
                        <div class="arch-merge-line"></div>
                        <div class="arch-arrow">â†“</div>
                    </div>
                </div>

                <!-- Add Noise -->
                <div class="arch-section" style="border-color: var(--accent-orange);">
                    <div class="arch-section-title">TRAINING: Add Noise</div>
                    <div class="arch-box orange" style="max-width: 100%; margin-top: 10px;">
                        <div class="arch-box-desc">noisy_trajectory = scheduler.add_noise(actions, Îµ, timestep)</div>
                    </div>
                </div>

                <!-- Arrow Down -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-arrow">â†“</div>
                </div>

                <!-- Conditional UNet -->
                <div class="arch-box blue" style="max-width: 100%;">
                    <div class="arch-box-title">Conditional UNet 1D</div>
                    <div class="arch-box-desc">Input: noisy_trajectory (B, horizon, action_dim)</div>
                    <div class="arch-box-desc">Condition: global_cond, timestep</div>
                    <div style="margin-top: 10px; padding: 10px; background: var(--bg-secondary); border-radius: 4px;">
                        <div class="arch-box-desc">â†“ Down blocks (Conv1d + GroupNorm + Attention)</div>
                        <div class="arch-box-desc">â†“ Mid block (Self-attention)</div>
                        <div class="arch-box-desc">â†“ Up blocks (Conv1d + GroupNorm + Skip connections)</div>
                        <div class="arch-box-desc">â†’ Output: predicted_noise or predicted_sample</div>
                    </div>
                </div>

                <!-- Arrow Down -->
                <div class="arch-row" style="justify-content: center;">
                    <div class="arch-arrow">â†“</div>
                </div>

                <!-- Loss Computation -->
                <div class="arch-box green" style="max-width: 100%;">
                    <div class="arch-box-title">Loss Computation</div>
                    <div class="arch-box-desc">if prediction_type == "epsilon": loss = MSE(predicted, Îµ)</div>
                    <div class="arch-box-desc">elif prediction_type == "sample": loss = MSE(predicted, actions)</div>
                </div>

                <!-- Inference Section -->
                <div class="arch-section" style="border-color: var(--accent-orange); margin-top: 30px;">
                    <div class="arch-section-title">INFERENCE: Iterative Denoising</div>
                    <div style="margin-top: 15px; padding: 15px; background: var(--bg-secondary); border-radius: 4px; font-family: 'JetBrains Mono', monospace; font-size: 0.85rem;">
                        <div style="color: var(--text-primary); margin-bottom: 8px;">sample = randn(B, horizon, action_dim)  # Start from pure noise</div>
                        <div style="color: var(--text-primary); margin-bottom: 8px;">for t in scheduler.timesteps:  # T â†’ 0</div>
                        <div style="color: var(--text-secondary); margin-left: 20px; margin-bottom: 4px;">model_output = unet(sample, t, global_cond)</div>
                        <div style="color: var(--text-secondary); margin-left: 20px; margin-bottom: 8px;">sample = scheduler.step(model_output, t, sample)  # Denoise step</div>
                        <div style="color: var(--text-primary);">return sample  # Clean action trajectory</div>
                    </div>
                </div>
            </div>
        </div>

        <h4>Diffusion Policy Forward Pass (Training)</h4>
        <pre><code><span class="keyword">def</span> <span class="function">compute_loss</span>(<span class="keyword">self</span>, batch):
    <span class="comment"># 1. Prepare global conditioning</span>
    global_cond = <span class="keyword">self</span>._prepare_global_conditioning(batch)
    <span class="comment"># global_cond shape: (B, n_obs_steps * (img_feat_dim + state_dim))</span>
    
    <span class="comment"># 2. Get target action trajectory</span>
    trajectory = batch[<span class="string">"action"</span>]  <span class="comment"># (B, horizon, action_dim)</span>
    
    <span class="comment"># 3. Sample random noise</span>
    eps = torch.randn(trajectory.shape, device=trajectory.device)
    
    <span class="comment"># 4. Sample random timesteps</span>
    timesteps = torch.randint(
        low=<span class="number">0</span>,
        high=<span class="keyword">self</span>.noise_scheduler.config.num_train_timesteps,
        size=(trajectory.shape[<span class="number">0</span>],),
        device=trajectory.device,
    ).long()
    
    <span class="comment"># 5. Add noise to trajectory</span>
    noisy_trajectory = <span class="keyword">self</span>.noise_scheduler.add_noise(trajectory, eps, timesteps)
    
    <span class="comment"># 6. Predict noise (or sample) with UNet</span>
    pred = <span class="keyword">self</span>.unet(noisy_trajectory, timesteps, global_cond=global_cond)
    
    <span class="comment"># 7. Compute loss</span>
    <span class="keyword">if</span> <span class="keyword">self</span>.config.prediction_type == <span class="string">"epsilon"</span>:
        target = eps  <span class="comment"># Predict noise</span>
    <span class="keyword">elif</span> <span class="keyword">self</span>.config.prediction_type == <span class="string">"sample"</span>:
        target = trajectory  <span class="comment"># Predict clean sample</span>
    
    loss = F.mse_loss(pred, target, reduction=<span class="string">"none"</span>)
    loss = (loss * ~batch[<span class="string">"action_is_pad"</span>].unsqueeze(-<span class="number">1</span>)).mean()
    
    <span class="keyword">return</span> loss</code></pre>

        <h4>RGB Encoder (ResNet Backbone)</h4>
        <pre><code><span class="keyword">class</span> <span class="class">DiffusionRgbEncoder</span>(nn.Module):
    <span class="keyword">def</span> <span class="function">__init__</span>(<span class="keyword">self</span>, config):
        <span class="comment"># Load pretrained ResNet</span>
        backbone_model = getattr(torchvision.models, config.vision_backbone)(
            weights=config.pretrained_backbone_weights
        )
        
        <span class="comment"># Remove classification head, keep feature extraction layers</span>
        <span class="keyword">self</span>.backbone = nn.Sequential(*list(backbone_model.children())[:-<span class="number">2</span>])
        
        <span class="comment"># Spatial softmax for extracting keypoints</span>
        <span class="keyword">self</span>.pool = SpatialSoftmax(...)
        
        <span class="comment"># Linear projection to feature dimension</span>
        <span class="keyword">self</span>.out = nn.Linear(num_kp * <span class="number">2</span>, config.diffusion_step_embed_dim)
    
    <span class="keyword">def</span> <span class="function">forward</span>(<span class="keyword">self</span>, x):
        <span class="comment"># x: (B, C, H, W)</span>
        x = <span class="keyword">self</span>.backbone(x)           <span class="comment"># (B, 512, H', W')</span>
        x = <span class="keyword">self</span>.pool(x)               <span class="comment"># (B, num_kp * 2)</span>
        x = F.relu(<span class="keyword">self</span>.out(x))       <span class="comment"># (B, embed_dim)</span>
        <span class="keyword">return</span> x</code></pre>
    </div>

    <!-- ============================================ -->
    <!-- SECTION 11: X-VLA -->
    <!-- ============================================ -->
    <h2 id="xvla">ğŸŒ 11. X-VLA: Cross-Embodiment VLA</h2>

    <h3>Architecture Overview</h3>
    
    <p>X-VLA (Cross-Embodiment Vision-Language-Action) uses Florence-2 as VLM backbone with a soft-prompted transformer for cross-robot generalization:</p>

    <div class="code-block">
        <pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              X-VLA Architecture                              â”‚
â”‚                      (Cross-Embodiment VLA)                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚    Multi-View Images â”€â”€â–º Florence-2 Vision Tower â”€â”€â–º Image Features â”€â”€â”     â”‚
â”‚                                                                        â”‚     â”‚
â”‚    Language â”€â”€â–º Florence-2 BART Tokenizer â”€â”€â–º Token Embeddings â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â–º  â”‚
â”‚                                                                        â”‚     â”‚
â”‚                         FLORENCE-2 ENCODER                                   â”‚
â”‚               (Vision + Language merged via multimodal projector)            â”‚
â”‚                              â”‚                                               â”‚
â”‚                              â–¼                                               â”‚
â”‚                    VLM Features (encoder output)                             â”‚
â”‚                              â”‚                                               â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚    â”‚                                                               â”‚        â”‚
â”‚    â–¼                                                               â–¼        â”‚
â”‚  Domain ID (embodiment identifier)                    Aux Visual Inputs     â”‚
â”‚    â”‚                                                               â”‚        â”‚
â”‚    â”‚          Proprioception State â”€â”€â–º Linear Projection          â”‚        â”‚
â”‚    â”‚                                   â”‚                           â”‚        â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚        â”‚
â”‚                           â–¼                                        â”‚        â”‚
â”‚               SOFT PROMPT HUB (Per-Domain Embediments)             â”‚        â”‚
â”‚             - 30 domains Ã— 32 learnable prompt tokens              â”‚        â”‚
â”‚             - Rapid adaptation to new robots                       â”‚        â”‚
â”‚                           â”‚                                        â”‚        â”‚
â”‚                           â–¼                                        â”‚        â”‚
â”‚               SOFT-PROMPTED TRANSFORMER                            â”‚        â”‚
â”‚             - 24 layers Ã— 16 heads Ã— 1024 hidden                   â”‚        â”‚
â”‚             - Flow matching for action generation                  â”‚        â”‚
â”‚             - Heterogeneous input projections                      â”‚        â”‚
â”‚                           â”‚                                        â”‚        â”‚
â”‚                           â–¼                                        â”‚        â”‚
â”‚            Action Denoising (Flow Matching)                        â”‚        â”‚
â”‚            x_t = t * noise + (1-t) * action                        â”‚        â”‚
â”‚            Predict velocity field: noise - action                  â”‚        â”‚
â”‚                           â”‚                                        â”‚        â”‚
â”‚                           â–¼                                        â”‚        â”‚
â”‚          ACTION HEAD (with Action Space Registry)                  â”‚        â”‚
â”‚          - ee6d: End-effector 6D + gripper (20D)                   â”‚        â”‚
â”‚          - so101_bimanual: SO101 bimanual (12D real â†’ 20D model)   â”‚        â”‚
â”‚          - auto: Auto-detect from dataset                          â”‚        â”‚
â”‚                           â”‚                                        â”‚        â”‚
â”‚                           â–¼                                        â”‚        â”‚
â”‚              Predicted Action Chunk (chunk_size=32)                â”‚        â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</code></pre>
    </div>

    <h3>Key Differences from Other VLAs</h3>
    
    <ul>
        <li><strong>VLM Backbone:</strong> Florence-2 (DaViT vision + BART language) vs PaliGemma/SmolVLM</li>
        <li><strong>Cross-Embodiment:</strong> Soft prompts per domain (30 robot types) for rapid adaptation</li>
        <li><strong>Action Registry:</strong> Pluggable action spaces (ee6d, so101_bimanual, auto)</li>
        <li><strong>Multi-View:</strong> Supports multiple camera views with auxiliary visual inputs</li>
        <li><strong>Freezing Strategy:</strong> Freeze VLM encoders, train only soft prompts + transformer</li>
        <li><strong>Training:</strong> Two-phase (pretraining on 290K episodes, then domain adaptation)</li>
    </ul>

    <div class="callout-warning">
        <strong>âš ï¸ Key Feature: Action Space Registry</strong><br>
        X-VLA uses an <strong>Action Registry</strong> system to handle different robots. Each action mode defines its own loss (MSE for joints, BCE for grippers). The <code>so101_bimanual</code> mode pads 12D real actions â†’ 20D model actions. The <code>auto</code> mode auto-detects action dim from dataset. This enables a single pretrained model to work across diverse robots.
    </div>

    <h3>Step-by-Step Forward Pass</h3>

    <h4>Step 1: Encode Vision-Language via Florence-2</h4>
    
    <div class="code-block">
        <pre><code><span class="keyword">def</span> <span class="function">forward_vlm</span>(<span class="keyword">self</span>, input_ids, pixel_values, image_mask):
    <span class="comment">"""Encode text and multi-view images via Florence-2 encoder."""</span>
    batch_size, num_views = pixel_values.shape[:2]
    
    <span class="comment"># 1a. Flatten and filter valid images</span>
    flat_images = pixel_values.flatten(0, 1)  <span class="comment"># (B*V, C, H, W)</span>
    flat_mask = image_mask.view(-1).to(dtype=torch.bool)
    valid_images = flat_images[flat_mask]
    
    <span class="comment"># 1b. Encode images with Florence-2 vision tower</span>
    valid_feats = <span class="keyword">self</span>.vlm._encode_image(valid_images)
    
    <span class="comment"># 1c. Reconstruct to (B, V, tokens, hidden)</span>
    image_features = valid_feats.new_zeros((batch_size * num_views, tokens_per_view, hidden_dim))
    image_features[flat_mask] = valid_feats
    image_features = image_features.view(batch_size, num_views, tokens_per_view, hidden_dim)
    
    <span class="comment"># 1d. Embed language tokens</span>
    inputs_embeds = <span class="keyword">self</span>.vlm.get_input_embeddings()(input_ids)
    
    <span class="comment"># 1e. Merge primary view with text via multimodal projector</span>
    merged_embeds, attention_mask = <span class="keyword">self</span>.vlm._merge_input_ids_with_image_features(
        image_features[:, 0],  <span class="comment"># Primary view</span>
        inputs_embeds,
    )
    
    <span class="comment"># 1f. Pass through Florence-2 BART encoder</span>
    enc_out = <span class="keyword">self</span>.vlm.language_model.model.encoder(
        attention_mask=attention_mask,
        inputs_embeds=merged_embeds,
    )[0]
    
    <span class="comment"># 1g. Prepare auxiliary views (views 1+)</span>
    aux_visual_inputs = image_features[:, 1:].reshape(batch_size, -1, hidden_dim)
    
    <span class="keyword">return</span> {<span class="string">"vlm_features"</span>: enc_out, <span class="string">"aux_visual_inputs"</span>: aux_visual_inputs}</code></pre>
    </div>

    <h4>Step 2: Flow Matching Training</h4>
    
    <div class="code-block">
        <pre><code><span class="keyword">def</span> <span class="function">forward</span>(<span class="keyword">self</span>, input_ids, image_input, image_mask, domain_id, proprio, action):
    <span class="comment">"""Forward pass for X-VLA model."""</span>
    <span class="comment"># 2a. Encode vision-language</span>
    enc = <span class="keyword">self</span>.forward_vlm(input_ids, image_input, image_mask)
    
    <span class="comment"># 2b. Sample random timestep for flow matching</span>
    batch_size = input_ids.shape[0]
    t = (torch.rand(1, device=device, dtype=dtype)
         + torch.arange(batch_size, device=device, dtype=dtype) / batch_size) % (1 - 1e-5)
    
    <span class="comment"># 2c. Flow matching interpolation: x_t = t * noise + (1-t) * action</span>
    action_noisy = torch.randn_like(action) * t.view(-1, 1, 1) + action * (1 - t).view(-1, 1, 1)
    
    <span class="comment"># 2d. Preprocess via action space (e.g., zero out grippers, pad dimensions)</span>
    proprio_m, action_noisy_m = <span class="keyword">self</span>.action_space.preprocess(proprio, action_noisy)
    
    <span class="comment"># 2e. Pass through soft-prompted transformer</span>
    pred_action = <span class="keyword">self</span>.transformer(
        domain_id=domain_id,          <span class="comment"># Which robot embodiment</span>
        action_with_noise=action_noisy_m,
        t=t,                          <span class="comment"># Flow matching timestep</span>
        proprio=proprio_m,
        **enc,                        <span class="comment"># VLM features + aux visuals</span>
    )
    
    <span class="comment"># 2f. Compute action space-specific loss</span>
    <span class="keyword">return</span> <span class="keyword">self</span>.action_space.compute_loss(pred_action, action)</code></pre>
    </div>

    <h4>Step 3: Inference with Flow Matching</h4>
    
    <div class="code-block">
        <pre><code><span class="keyword">@torch.no_grad</span>()
<span class="keyword">def</span> <span class="function">generate_actions</span>(<span class="keyword">self</span>, input_ids, image_input, image_mask, domain_id, proprio, steps):
    <span class="comment">"""Generate actions via flow matching denoising."""</span>
    <span class="keyword">self</span>.eval()
    
    <span class="comment"># 3a. Encode vision-language (once)</span>
    enc = <span class="keyword">self</span>.forward_vlm(input_ids, image_input, image_mask)
    
    <span class="comment"># 3b. Start from random noise</span>
    batch_size = input_ids.shape[0]
    x1 = torch.randn(batch_size, <span class="keyword">self</span>.chunk_size, action_dim, device=device)
    action = torch.zeros_like(x1)
    
    <span class="comment"># 3c. Iterative denoising (default: 10 steps)</span>
    steps = max(1, int(steps))
    <span class="keyword">for</span> i <span class="keyword">in</span> range(steps, 0, -1):
        t = torch.full((batch_size,), i / steps, device=device)
        
        <span class="comment"># Interpolate: x_t = t * x1 + (1-t) * action</span>
        x_t = x1 * t.view(-1, 1, 1) + action * (1 - t).view(-1, 1, 1)
        
        <span class="comment"># Preprocess and predict</span>
        proprio_m, x_t_m = <span class="keyword">self</span>.action_space.preprocess(proprio, x_t)
        action = <span class="keyword">self</span>.transformer(
            domain_id=domain_id,
            action_with_noise=x_t_m,
            proprio=proprio_m,
            t=t,
            **enc,
        )
    
    <span class="comment"># 3d. Postprocess (e.g., apply sigmoid to grippers, trim padding)</span>
    <span class="keyword">return</span> <span class="keyword">self</span>.action_space.postprocess(action)</code></pre>
    </div>

    <h3>ğŸ”‘ Key Concept: Soft Prompt Hub (Cross-Embodiment)</h3>
    
    <p>X-VLA achieves cross-embodiment generalization via <strong>soft prompt hub</strong>:</p>

    <div class="code-block">
        <pre><code><span class="comment"># Soft Prompt Hub: 30 domains Ã— 32 learnable tokens per domain</span>
<span class="keyword">self</span>.soft_prompt_hub = nn.Parameter(torch.zeros(num_domains, len_soft_prompts, hidden_size))

<span class="comment"># At runtime, select prompts for specific embodiment</span>
domain_prompts = <span class="keyword">self</span>.soft_prompt_hub[domain_id]  <span class="comment"># (B, 32, 1024)</span>

<span class="comment"># These prompts are prepended to transformer input</span>
<span class="comment"># They encode embodiment-specific information (kinematics, workspace, etc.)</span></code></pre>
    </div>

    <p><strong>Benefits:</strong></p>
    <ul>
        <li><strong>Rapid Adaptation:</strong> Fine-tune only 32 prompts (32K params) for new robot</li>
        <li><strong>Frozen Backbone:</strong> VLM encoders stay frozen, preserving pretrained knowledge</li>
        <li><strong>Data Efficient:</strong> 3K steps sufficient to adapt to new embodiment</li>
        <li><strong>Scalability:</strong> Support 30+ robot types with single model</li>
    </ul>

    <h3>ğŸ”‘ Key Concept: Action Space Registry</h3>
    
    <p>X-VLA uses an <strong>Action Registry</strong> to handle different action spaces:</p>

    <div class="code-block">
        <pre><code><span class="comment"># Example: SO101 Bimanual (12D real â†’ 20D model)</span>
<span class="keyword">@register_action</span>(<span class="string">"so101_bimanual"</span>)
<span class="keyword">class</span> <span class="class-name">BimanualSO101ActionSpace</span>(BaseActionSpace):
    dim_action = 20          <span class="comment"># Model output dimension</span>
    REAL_DIM = 12           <span class="comment"># Real robot dimension</span>
    gripper_idx = (5, 11)   <span class="comment"># Gripper indices</span>
    
    <span class="keyword">def</span> <span class="function">preprocess</span>(<span class="keyword">self</span>, proprio, action):
        <span class="comment">"""Pad 12D real actions to 20D for model."""</span>
        action = pad_to_20d(action)
        <span class="comment"># Zero out gripper channels during training</span>
        action[:, :, <span class="keyword">self</span>.gripper_idx] = 0
        <span class="keyword">return</span> proprio, action
    
    <span class="keyword">def</span> <span class="function">compute_loss</span>(<span class="keyword">self</span>, pred, target):
        <span class="comment">"""MSE for joints, BCE for grippers."""</span>
        joint_loss = F.mse_loss(pred[:, :, joint_idxs], target[:, :, joint_idxs])
        gripper_loss = F.binary_cross_entropy_with_logits(
            pred[:, :, gripper_idx], target[:, :, gripper_idx]
        )
        <span class="keyword">return</span> {<span class="string">"joint_loss"</span>: joint_loss, <span class="string">"gripper_loss"</span>: gripper_loss}
    
    <span class="keyword">def</span> <span class="function">postprocess</span>(<span class="keyword">self</span>, action):
        <span class="comment">"""Trim 20D model output to 12D real, apply sigmoid to grippers."""</span>
        action = action[:, :<span class="keyword">self</span>.REAL_DIM]  <span class="comment"># Trim padding</span>
        action[:, <span class="keyword">self</span>.gripper_idx] = torch.sigmoid(action[:, <span class="keyword">self</span>.gripper_idx])
        <span class="keyword">return</span> action</code></pre>
    </div>

    <h3>Training Configuration</h3>
    
    <div class="code-block">
        <pre><code><span class="comment"># Two-phase training strategy</span>
<span class="comment"># Phase I: Pretraining (290K episodes, multi-embodiment)</span>
lerobot-train \
    --policy.type=xvla \
    --policy.path=lerobot/xvla-base \
    --dataset.repo_id=multi_embodiment_dataset \
    --policy.freeze_vision_encoder=true \
    --policy.freeze_language_encoder=true \
    --policy.train_soft_prompts=true \
    --steps=100000

<span class="comment"># Phase II: Domain Adaptation (3K steps, target robot)</span>
lerobot-train \
    --policy.type=xvla \
    --policy.path=lerobot/xvla-base \
    --dataset.repo_id=bimanual_so101_dataset \
    --policy.action_mode=so101_bimanual \
    --policy.freeze_vision_encoder=false \  <span class="comment"># Unfreeze for best performance</span>
    --policy.freeze_language_encoder=false \
    --policy.train_soft_prompts=true \
    --steps=3000</code></pre>
    </div>

    <h3>Differential Learning Rates</h3>
    
    <p>X-VLA uses <strong>XVLAAdamW</strong> optimizer with differential LRs:</p>

    <div class="code-block">
        <pre><code><span class="comment"># Optimizer applies different learning rates based on parameter names</span>
optimizer = XVLAAdamW(
    model.get_optim_params(),
    lr=1e-4,                                    <span class="comment"># Base LR</span>
    betas=(0.9, 0.99),
    weight_decay=0.0,
)

<span class="comment"># Learning rate scheme:</span>
<span class="comment"># - VLM parameters (vision/language encoders): lr / 10 = 1e-5</span>
<span class="comment"># - Soft prompts: lr * soft_prompt_lr_scale = 1e-4 (default)</span>
<span class="comment"># - Transformer/action head: lr = 1e-4</span>

<span class="comment"># This ensures stable optimization of pretrained VLM while allowing</span>
<span class="comment"># rapid adaptation of task-specific components</span></code></pre>
    </div>

    <!-- ============================================ -->
    <!-- SECTION 12: Comparison -->
    <!-- ============================================ -->
    <h2 id="comparison">ğŸ“Š 12. Model Comparison Summary</h2>

    <table class="comparison-table">
        <thead>
            <tr>
                <th>Feature</th>
                <th>SmolVLA</th>
                <th>Pi0</th>
                <th>Pi0.5</th>
                <th>Pi0.5-Fast</th>
                <th>ACT</th>
                <th>Groot</th>
                <th>Diffusion</th>
                <th>X-VLA</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><strong>Vision Encoder</strong></td>
                <td>SigLIP (SmolVLM)</td>
                <td>SigLIP (PaliGemma)</td>
                <td>SigLIP (PaliGemma)</td>
                <td>SigLIP (PaliGemma)</td>
                <td>ResNet18</td>
                <td>Eagle VL</td>
                <td>ResNet (configurable)</td>
                <td>DaViT (Florence-2)</td>
            </tr>
            <tr>
                <td><strong>Language Model</strong></td>
                <td>SmolVLM2 LLM</td>
                <td>Gemma 2B</td>
                <td>Gemma 2B</td>
                <td>Gemma 2B</td>
                <td>None</td>
                <td>Eagle LLM</td>
                <td>None</td>
                <td>BART (Florence-2)</td>
            </tr>
            <tr>
                <td><strong>Action Generation</strong></td>
                <td>Flow Matching</td>
                <td>Flow Matching</td>
                <td>Flow Matching</td>
                <td>Flow Matching</td>
                <td>Encoder-Decoder</td>
                <td>Flow Matching (DiT)</td>
                <td>DDPM/DDIM Denoising</td>
                <td>Flow Matching</td>
            </tr>
            <tr>
                <td><strong>Image Resolution</strong></td>
                <td>512Ã—512</td>
                <td>224Ã—224</td>
                <td>224Ã—224</td>
                <td>224Ã—224</td>
                <td>96Ã—96 (configurable)</td>
                <td>Variable (Eagle)</td>
                <td>96Ã—96 (configurable)</td>
                <td>Variable (resizable)</td>
            </tr>
            <tr>
                <td><strong>Default chunk_size</strong></td>
                <td>50</td>
                <td>50</td>
                <td>50</td>
                <td>50</td>
                <td>100</td>
                <td>16</td>
                <td>Variable (horizon)</td>
                <td>32</td>
            </tr>
            <tr>
                <td><strong>Loss Function</strong></td>
                <td>MSE (velocity)</td>
                <td>MSE (velocity)</td>
                <td>MSE (velocity)</td>
                <td>MSE (velocity)</td>
                <td>L1 + KL (VAE)</td>
                <td>MSE (velocity)</td>
                <td>MSE (noise/sample)</td>
                <td>Action-space specific</td>
            </tr>
            <tr>
                <td><strong>Normalization</strong></td>
                <td>Mean-Std (state/action)</td>
                <td>Mean-Std (state/action)</td>
                <td>Quantiles (state/action)</td>
                <td>Quantiles (state/action)</td>
                <td>Mean-Std (all)</td>
                <td>Min-Max</td>
                <td>Mean-Std (all)</td>
                <td>Identity (ImageNet for vision)</td>
            </tr>
            <tr>
                <td><strong>Language Conditioning</strong></td>
                <td>âœ… Yes</td>
                <td>âœ… Yes</td>
                <td>âœ… Yes</td>
                <td>âœ… Yes</td>
                <td>âŒ No</td>
                <td>âœ… Yes</td>
                <td>âŒ No</td>
                <td>âœ… Yes</td>
            </tr>
            <tr>
                <td><strong>Robot State Input</strong></td>
                <td>âœ… PREFIX (state_proj)</td>
                <td>âœ… SUFFIX (state_proj)</td>
                <td>âš ï¸ Tokenized in prompt</td>
                <td>âš ï¸ Tokenized in prompt</td>
                <td>âœ… Yes</td>
                <td>âœ… Yes</td>
                <td>âœ… Yes</td>
                <td>âœ… Linear projection</td>
            </tr>
            <tr>
                <td><strong>Expert Size</strong></td>
                <td>Lightweight</td>
                <td>Gemma 300M</td>
                <td>Gemma 300M</td>
                <td>Gemma 300M</td>
                <td>N/A</td>
                <td>DiT</td>
                <td>UNet</td>
                <td>Transformer (24L, 16H, 1024D)</td>
            </tr>
            <tr>
                <td><strong>Tokenizer Length</strong></td>
                <td>48 tokens</td>
                <td>48 tokens</td>
                <td>200 tokens</td>
                <td>200 tokens</td>
                <td>N/A</td>
                <td>Variable</td>
                <td>N/A</td>
                <td>64 tokens (BART)</td>
            </tr>
            <tr>
                <td><strong>Finetuning Strategy</strong></td>
                <td>Expert only (freeze VLM)</td>
                <td>Expert only (freeze VLM)</td>
                <td>Expert only (freeze VLM)</td>
                <td>Expert only (freeze VLM)</td>
                <td>Full model</td>
                <td>Configurable</td>
                <td>Full model</td>
                <td>Soft prompts + Transformer</td>
            </tr>
            <tr>
                <td><strong>Inference Steps</strong></td>
                <td>10 (flow steps)</td>
                <td>10 (flow steps)</td>
                <td>10 (flow steps)</td>
                <td>5 (flow steps)</td>
                <td>1 (direct)</td>
                <td>10 (flow steps)</td>
                <td>100 (denoising)</td>
                <td>10 (flow steps)</td>
            </tr>
            <tr>
                <td><strong>Cross-Embodiment</strong></td>
                <td>âŒ No</td>
                <td>âŒ No</td>
                <td>âŒ No</td>
                <td>âŒ No</td>
                <td>âŒ No</td>
                <td>âœ… Yes</td>
                <td>âŒ No</td>
                <td>âœ… Yes (Soft prompts)</td>
            </tr>
            <tr>
                <td><strong>Action Space Registry</strong></td>
                <td>âŒ No</td>
                <td>âŒ No</td>
                <td>âŒ No</td>
                <td>âŒ No</td>
                <td>âŒ No</td>
                <td>âŒ No</td>
                <td>âŒ No</td>
                <td>âœ… Yes (Pluggable)</td>
            </tr>
            <tr>
                <td><strong>Multi-View Support</strong></td>
                <td>âŒ No</td>
                <td>âŒ No</td>
                <td>âŒ No</td>
                <td>âŒ No</td>
                <td>âœ… Yes</td>
                <td>âœ… Yes</td>
                <td>âŒ No</td>
                <td>âœ… Yes</td>
            </tr>
        </tbody>
    </table>

    <div class="key-concept">
        <h4>ğŸ¯ Choosing the Right Model</h4>
        <ul>
            <li><strong>SmolVLA:</strong> Best for language-conditioned tasks with limited compute. Lightweight and fast.</li>
            <li><strong>Pi0:</strong> High-quality VLA with strong generalization. Uses robot state. Good for complex manipulation tasks.</li>
            <li><strong>Pi0.5:</strong> Open-world generalization without robot state. Uses AdaRMS conditioning. Longer language instructions (200 tokens). <strong>97.5% LIBERO benchmark.</strong></li>
            <li><strong>Pi0.5-Fast:</strong> Pi0.5 with 5 inference steps instead of 10. ~2x faster inference, slightly lower quality.</li>
            <li><strong>ACT:</strong> Best for bimanual manipulation (Aloha). Fast inference, simple architecture.</li>
            <li><strong>Groot:</strong> Multi-embodiment support. Good for transferring policies across robots. <strong>87% LIBERO benchmark.</strong></li>
            <li><strong>Diffusion:</strong> Versatile and proven. Good for multi-modal action distributions.</li>
            <li><strong>X-VLA:</strong> <strong>Cross-embodiment champion.</strong> Florence-2 backbone + soft prompts for rapid adaptation. Pluggable action spaces (ee6d, so101_bimanual, auto). <strong>93% LIBERO, 100% cloth folding.</strong> Best for transferring across diverse robots or bimanual SO101.</li>
        </ul>
    </div>

    <hr style="margin: 50px 0; border-color: var(--border-color);">
    <p style="text-align: center; color: var(--text-secondary);">
        Generated for LeRobot Training Pipeline Understanding<br>
        <a href="https://github.com/huggingface/lerobot" style="color: var(--accent-blue);">https://github.com/huggingface/lerobot</a>
    </p>
</body>
</html>

